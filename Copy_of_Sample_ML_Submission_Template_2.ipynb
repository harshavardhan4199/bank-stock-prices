{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "Yfr_Vlr8HBkt",
        "tEA2Xm5dHt1r",
        "Ou-I18pAyIpj",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "kLW572S8pZyI",
        "578E2V7j08f6",
        "67NQN5KX2AMe",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshavardhan4199/bank-stock-prices/blob/main/Copy_of_Sample_ML_Submission_Template_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Shopper Spectrum: Customer Segmentation and Product Recommendations in E-Commerce\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** N.Harsha Vardhan\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The “Shopper Spectrum: Customer Segmentation and Product Recommendations in E-Commerce” dataset offers a comprehensive view of online retail transactions, capturing the behavior and preferences of customers across various countries. With over 541,000 entries, each row represents an individual product purchase, including key attributes such as invoice number, product code and description, quantity, invoice date, unit price, customer ID, and country. This granular data enables a wide range of analytical applications, from customer profiling to product trend analysis. While the majority of transactions are from the United Kingdom, the dataset also includes purchases from other European and international regions, allowing for a comparative analysis of geographic shopping patterns.\n",
        "\n",
        "One of the primary uses of this dataset is in customer segmentation. By analyzing metrics such as purchase frequency, total spend, and recency of transactions—commonly known as RFM analysis—retailers can categorize customers into groups like loyal shoppers, occasional buyers, and price-sensitive customers. This segmentation is instrumental in personalizing marketing efforts and improving customer retention. Additionally, the presence of product-level data tied to specific invoices allows for market basket analysis, identifying frequently co-purchased items and enabling cross-selling opportunities. These insights form the foundation for collaborative and content-based recommendation systems that suggest products based on a customer’s history or item similarities.\n",
        "\n",
        "Another key application of the dataset lies in sales and trend analysis. The timestamped invoice data enables the identification of seasonal trends, peak shopping periods, and possible stock management issues. For instance, products with consistently high quantities or those showing spikes in specific months can guide inventory planning. The dataset also includes returns, identified by negative quantities or unit prices, which can be used to assess product quality or customer satisfaction issues.\n",
        "\n",
        "Despite its strengths, the dataset presents challenges such as missing customer IDs for about 25% of transactions and some null product descriptions. These issues can limit the effectiveness of certain personalized models but are manageable with data preprocessing and imputation techniques. Nevertheless, the overall richness of the data provides immense value for building machine learning models aimed at enhancing user experience, optimizing sales strategies, and improving operational efficiency in e-commerce.\n",
        "\n",
        "In summary, the “Shopper Spectrum” dataset is a powerful resource for businesses looking to leverage data-driven decision-making. It supports the development of robust customer segmentation strategies, intelligent recommendation systems, and detailed market insights. With thoughtful analysis and appropriate modeling techniques, this dataset can significantly enhance a retailer’s ability to understand and serve its customers in a competitive online environment."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the dynamic and competitive world of e-commerce, understanding customer behavior and delivering personalized shopping experiences have become critical for business success. However, the vast diversity in customer preferences, buying patterns, and geographical differences makes it challenging for retailers to design effective marketing and sales strategies. This project addresses the need for intelligent customer segmentation and product recommendation systems using the “Shopper Spectrum” dataset. The primary objective is to categorize customers based on their purchasing frequency, recency, and monetary value to enable targeted marketing approaches. Additionally, the project aims to develop recommendation systems that suggest relevant products to customers by analyzing their past purchases or the behavior of similar users. Another key focus is on identifying sales trends over time to support better inventory management and promotional planning. The dataset also presents real-world challenges such as missing customer IDs and incomplete product descriptions, which must be carefully managed to ensure reliable insights. Overall, the project seeks to provide data-driven solutions that empower e-commerce businesses to enhance customer engagement, improve operational efficiency, and drive growth through smarter decision-making."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv(\"online_retail.csv\", encoding='ISO-8859-1')\n",
        "\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "# Import necessary library\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"online_retail.csv\", encoding='ISO-8859-1')\n",
        "\n",
        "# Dataset First Look - Display top 5 rows\n",
        "print(\"Dataset First Look (Top 5 Rows):\\n\")\n",
        "print(df.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "missing = df.isnull().sum()\n",
        "missing = missing[missing > 0]\n",
        "\n",
        "missing.plot(kind='bar', figsize=(8, 4), color='salmon')\n",
        "plt.title(\"Missing Values per Column\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Online Retail dataset is a rich and detailed collection of transactional records from a UK-based e-commerce platform, consisting of 541,909 rows and 8 columns. Each entry in the dataset represents an individual product purchased in a transaction and includes information such as invoice number, product code, product description, quantity purchased, invoice date, unit price, customer ID, and the customer’s country. This dataset provides a comprehensive view of customer purchasing behavior over time, making it highly suitable for analysis in customer segmentation, product recommendation, and sales trend forecasting.\n",
        "\n",
        "Initial exploration reveals that while the majority of transactions originate from the United Kingdom, the dataset also includes sales from various international markets, enabling cross-country analysis. A significant challenge in the dataset is the presence of missing values, particularly in the CustomerID column, which is missing in over 135,000 records—about 25% of the data. The Description field also has some null values. These issues should be addressed during data cleaning to ensure accurate analysis. Additionally, the dataset contains 5,268 duplicate entries, which can distort insights if not removed.\n",
        "\n",
        "The InvoiceDate field, stored as a string, should be converted to a datetime format for time-based analysis. Negative values in the Quantity and UnitPrice columns indicate product returns or corrections and must be handled appropriately during preprocessing. Overall, this dataset captures a broad spectrum of retail transactions and offers substantial opportunities for deriving business insights, such as identifying high-value customers, understanding seasonal trends, and implementing personalized recommendation systems."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns.tolist()"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Online Retail dataset contains eight key variables that provide detailed insights into customer transactions on an e-commerce platform. Each row represents a product-level entry in a customer invoice. The InvoiceNo column is an alphanumeric identifier that uniquely represents each transaction; if it starts with the letter 'C', it usually indicates a cancellation or return. StockCode refers to the unique code assigned to each product, while Description provides the name or short detail of the product purchased. The Quantity column indicates the number of units purchased per product per invoice, where negative values typically represent product returns.\n",
        "\n",
        "The InvoiceDate column captures the exact date and time when the transaction occurred, though it is initially in object (string) format and should be converted to datetime for temporal analysis. UnitPrice indicates the price per unit of the product in British Pounds, and negative prices may also represent adjustments or returns. The CustomerID is a numeric, anonymized identifier assigned to each customer; however, a significant number of rows have missing values in this column, indicating unidentified or guest customers. Finally, the Country column specifies the customer’s location at the time of purchase, which can be used for regional or geographic analysis. Altogether, these variables support a wide range of analyses, including customer segmentation, sales trend forecasting, and personalized product recommendation systems."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"online_retail.csv\", encoding='ISO-8859-1')\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Remove rows with missing CustomerID or Description\n",
        "df.dropna(subset=['CustomerID', 'Description'], inplace=True)\n",
        "\n",
        "# Remove rows with non-positive Quantity or UnitPrice\n",
        "df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "df['CustomerID'] = df['CustomerID'].astype(str)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "# Show final shape and sample data\n",
        "print(\" Cleaned Dataset Shape:\", df.shape)\n",
        "print(\" Sample Data:\\n\", df.head())\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During the preprocessing and cleaning of the Online Retail dataset, several key manipulations were performed to ensure the data was ready for analysis. First, all duplicate records were removed to avoid redundancy and ensure accuracy in statistical analysis and modeling. Next, rows with missing values in critical columns such as CustomerID and Description were dropped. These fields are essential for tasks like customer segmentation and product recommendation, and keeping records with missing values would have hindered those efforts. Additionally, transactions with non-positive Quantity or UnitPrice values—typically representing returns, errors, or cancellations—were also removed to maintain data integrity for sales and revenue analysis.\n",
        "\n",
        "To facilitate time-based analysis, the InvoiceDate column was converted from string to datetime format. This enables the dataset to be used for monthly trend analysis, seasonality detection, and revenue forecasting. The CustomerID column was converted from a float to a string type, treating it as a categorical identifier rather than a numerical value. To enhance analytical capabilities, a new derived column TotalPrice was created by multiplying Quantity by UnitPrice, providing a direct measure of revenue generated per transaction. Finally, after all transformations and filters, the DataFrame index was reset for consistency.\n",
        "\n",
        "From these manipulations, several insights emerged. It was observed that a significant portion of the transactions originated from the United Kingdom, suggesting a primarily domestic customer base. The data also indicated a wide range of customer purchasing behaviors—some customers made large, repeated purchases, indicative of wholesale buyers, while others made sporadic, smaller purchases, suitable for targeting in customer loyalty programs. Popular products were identified through repeated appearances in the StockCode and Description columns, providing insights into high-demand inventory. The newly created TotalPrice column enabled the calculation of total revenue and revealed high-value customers and peak transaction periods. Moreover, time-based patterns could be identified using the cleaned InvoiceDate column, paving the way for seasonal trend analysis."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Group by Country and count transactions\n",
        "country_orders = df[df['Country'] != 'United Kingdom']['Country'].value_counts().head(10)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=country_orders.values, y=country_orders.index, palette=\"viridis\")\n",
        "plt.title('Top 10 Countries by Number of Transactions (Excluding UK)')\n",
        "plt.xlabel('Number of Transactions')\n",
        "plt.ylabel('Country')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bar chart was chosen because it is ideal for comparing categorical data such as country-wise transaction counts. In the context of the Online Retail dataset, understanding the geographical distribution of transactions is crucial. The United Kingdom dominates the data, so excluding it allows us to focus on international customers. A bar chart clearly illustrates which non-UK countries generate the most activity, making it easier to identify potential international markets and evaluate global business performance at a glance.\n",
        "\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the chart, we discovered that countries like the Netherlands, Germany, France, Ireland, and Norway had the highest number of transactions after the UK. This indicates that these countries represent the most active international markets for the business. The insight also shows that while international sales are present, they are significantly lower in volume compared to domestic (UK) sales. This suggests the business has an established presence in the UK and room to grow internationally.\n",
        "\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, these insights can help create a positive business impact. By identifying the top-performing countries outside the UK, the business can:\n",
        "\n",
        "Strategically target marketing campaigns toward countries with existing customer engagement.\n",
        "\n",
        "Explore localized promotions or shipping improvements in those regions to boost international sales.\n",
        "\n",
        "Invest in language-specific customer experiences or dedicated regional websites.\n",
        "\n",
        "There are no direct insights in this chart that point to negative growth, but there is a potential concern: the high dependency on the UK market. Over-reliance on one country can expose the business to regional risks such as economic slowdowns, policy changes like Brexit, or supply chain disruptions. This concentration risk could lead to negative growth if not balanced with international market development.\n",
        "\n",
        "In conclusion, while the chart highlights positive opportunities in global growth, it also indirectly signals a need to diversify beyond a UK-centric business model to ensure long-term stability and scalability."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure InvoiceDate is in datetime format\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "\n",
        "# Extract Year-Month for grouping\n",
        "df['YearMonth'] = df['InvoiceDate'].dt.to_period('M')\n",
        "\n",
        "# Calculate monthly revenue\n",
        "monthly_revenue = df.groupby('YearMonth')['TotalPrice'].sum().reset_index()\n",
        "monthly_revenue['YearMonth'] = monthly_revenue['YearMonth'].astype(str)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(monthly_revenue['YearMonth'], monthly_revenue['TotalPrice'], marker='o', linestyle='-')\n",
        "plt.title('Monthly Revenue Trend')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Revenue (£)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line chart was selected because it is the most effective way to represent continuous data over time, such as monthly revenue. It helps visualize trends, seasonality, and fluctuations in a clear and chronological manner. This is particularly useful in retail analysis, where understanding sales performance over months is critical for making data-driven decisions like inventory planning, marketing timing, and budgeting. The line chart makes it easy to spot both upward and downward trends in revenue."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the monthly revenue trend chart, we can uncover several important insights:\n",
        "\n",
        "Certain months show clear revenue spikes, which may correspond to peak shopping seasons like holidays or end-of-year sales.\n",
        "\n",
        "Some months display a drop in revenue, possibly due to seasonality, product unavailability, customer returns, or reduced marketing activity.\n",
        "\n",
        "Overall, the chart can indicate whether the business is growing over time or experiencing inconsistencies in monthly performance.\n",
        "\n",
        "Such patterns help the business understand when customers are most active and when revenue tends to dip, enabling more efficient planning."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights can significantly contribute to positive business impact:\n",
        "\n",
        "By identifying peak months, the business can concentrate promotional efforts, optimize inventory, and staff operations accordingly.\n",
        "\n",
        "Recognizing low-revenue months gives the business a chance to introduce seasonal discounts, new campaigns, or customer engagement initiatives to boost sales.\n",
        "\n",
        "Over time, tracking this chart enables better forecasting and resource allocation, reducing wastage and increasing profitability.\n",
        "\n",
        "However, the chart may also highlight negative growth signals. For example:\n",
        "\n",
        "Sharp revenue drops could point to operational issues such as stockouts, delivery delays, or high return rates.\n",
        "\n",
        "Lack of growth or a declining trend over several months may indicate market saturation, customer churn, or ineffective marketing."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Group by Description and count transactions\n",
        "top_products = df['Description'].value_counts().head(10)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=top_products.values, y=top_products.index, palette=\"magma\")\n",
        "plt.title('Top 10 Most Frequently Purchased Products')\n",
        "plt.xlabel('Number of Purchases')\n",
        "plt.ylabel('Product Description')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart was chosen because it effectively compares the frequency of discrete categories—in this case, product descriptions. When analyzing retail or e-commerce data, it is essential to identify which products are most popular among customers. Bar charts make it easy to visually distinguish which items are top-performers and by how much they outperform others. This format allows for quick interpretation of which products are driving the most transactions, which is crucial for inventory and marketing decisions.\n",
        "\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the chart, we observe that a few products significantly outpace others in terms of purchase frequency. These are the \"hero products\"—they are in high demand and likely play a key role in attracting customers. For example, items like \"White Hanging Heart T-Light Holder\" or \"Regency Cakestand 3 Tier\" (commonly seen in this dataset) might top the chart due to their popularity in home decor.\n",
        "\n",
        "These insights show that:\n",
        "\n",
        "The business has specific products that act as key revenue drivers.\n",
        "\n",
        "Some products may have seasonal popularity or appeal to a broader audience.\n",
        "\n",
        "Understanding these preferences can help with demand forecasting, cross-selling, and strategic stocking.\n",
        "\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, these insights are highly beneficial for creating a positive business impact:\n",
        "\n",
        "By knowing the most frequently bought products, businesses can optimize inventory, ensuring that best-sellers are always in stock.\n",
        "\n",
        "These items can be featured in bundling strategies, homepage promotions, and email campaigns to increase average order value.\n",
        "\n",
        "It also helps in product development, as similar products can be introduced based on proven popularity.\n",
        "\n",
        "However, there may be potential risks if not managed properly:\n",
        "\n",
        "Over-dependence on a few products can be risky. If demand suddenly drops or supply chain issues arise for those items, it could lead to negative growth.\n",
        "\n",
        "If many sales come from low-margin products, the business might see high volume but low profitability, which can affect long-term sustainability."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Filter out extreme values for clearer visualization\n",
        "filtered_df = df[(df['Quantity'] > 0) & (df['Quantity'] < 100)]\n",
        "\n",
        "# Plotting the distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(filtered_df['Quantity'], bins=50, kde=True, color='teal')\n",
        "plt.title('Distribution of Purchase Quantities')\n",
        "plt.xlabel('Quantity Purchased')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram was selected because it is ideal for showing the distribution of continuous numerical variables, such as quantity purchased per transaction. It allows us to visualize the spread and concentration of order sizes across the dataset. By plotting the purchase quantities, we can observe common ordering patterns, identify extreme values (bulk purchases or potential errors), and better understand customer behavior. This chart is essential for inventory and operations teams to determine how to stock and fulfill typical order volumes."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the histogram, we can draw several insights:\n",
        "\n",
        "The majority of purchases involve small quantities (e.g., between 1 and 10 units), which is expected in a retail or gift-oriented e-commerce business.\n",
        "\n",
        "There are a few higher quantity orders, suggesting the presence of bulk buyers (such as resellers or corporate clients).\n",
        "\n",
        "By removing outliers (e.g., quantities over 100 or negative values), we cleaned the data for more meaningful analysis.\n",
        "\n",
        "The long tail of the distribution also highlights occasional large-volume transactions, which could be strategic accounts or seasonal spikes."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights from this chart are valuable and can lead to positive business impact:\n",
        "\n",
        "Helps the business optimize inventory to match the most common order sizes.\n",
        "\n",
        "Enables better packaging and logistics planning, particularly for small vs. bulk orders.\n",
        "\n",
        "Supports customer segmentation based on purchase behavior, which can be used for personalized marketing (e.g., bulk buyers can be offered wholesale discounts).\n",
        "\n",
        "However, the chart also exposes possible risks that may lead to negative growth if not addressed:\n",
        "\n",
        "Negative or unusually high quantities (possibly due to data entry errors or return transactions) may distort performance metrics and should be flagged and cleaned.\n",
        "\n",
        "Overlooking the demand from bulk buyers could result in missed opportunities if their needs are not separately addressed.\n",
        "\n",
        "If the business misjudges the proportion of small vs. large orders, it may face issues like stockouts or overstock, leading to either lost sales or increased holding costs."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Selecting relevant numerical features\n",
        "numeric_features = df[['Quantity', 'UnitPrice', 'TotalPrice']]\n",
        "\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = numeric_features.corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "plt.title('Correlation Heatmap of Numeric Features')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation heatmap was chosen because it visually summarizes the strength and direction of relationships between multiple numerical variables—specifically Quantity, UnitPrice, and TotalPrice. This chart is essential when we want to understand how features behave with respect to each other, especially in large transactional datasets. The heatmap makes it easy to identify positive or negative correlations and spot variables that may be redundant or influential in future predictive modeling or business strategies.\n",
        "\n"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the heatmap, we generally find the following patterns:\n",
        "\n",
        "A strong positive correlation between Quantity and TotalPrice, which is expected—more quantity typically increases total value.\n",
        "\n",
        "A moderate correlation between UnitPrice and TotalPrice, depending on pricing distribution.\n",
        "\n",
        "In some cases, there may be a negative or weak correlation between Quantity and UnitPrice, suggesting that lower-priced items are often bought in bulk, whereas higher-priced items are bought in smaller quantities."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "\n",
        "The heatmap allows for better feature engineering if machine learning is to be used for recommendations, forecasting, or segmentation.\n",
        "\n",
        "It shows which variables are strongly related—helping analysts prioritize which features to monitor or optimize.\n",
        "\n",
        "The inverse relationship between UnitPrice and Quantity may suggest an opportunity to introduce discount-based bundling to move high-priced items.\n",
        "\n",
        "Possible Risk/Negative Growth:\n",
        "\n",
        "If the business relies too heavily on items with low margins but high sales volume, profitability could suffer despite strong correlations with total sales.\n",
        "\n",
        "Misinterpreting correlation as causation could lead to ineffective strategies (e.g., assuming increasing unit price will proportionally increase revenue, without considering its effect on quantity purchased)."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Grouping by product description and summing quantities\n",
        "top_products = df.groupby('Description')['Quantity'].sum().sort_values(ascending=False).head(10)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=top_products.values, y=top_products.index, palette='viridis')\n",
        "plt.title('Top 10 Most Purchased Products')\n",
        "plt.xlabel('Total Quantity Purchased')\n",
        "plt.ylabel('Product Description')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A horizontal bar chart was chosen because it is highly effective in visualizing categorical variables with quantitative values—in this case, product descriptions against the total quantities sold. It allows for a clear, intuitive comparison between product demand levels and is especially useful when dealing with long product names. This type of chart highlights the most popular items, which can directly inform decisions in inventory management, product recommendation, and marketing."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this chart, we gain several key insights:\n",
        "\n",
        "The top 10 products significantly outperform others in terms of purchase volume.\n",
        "\n",
        "There may be recurring product themes among the top sellers (e.g., seasonal items, home décor, or novelty items), hinting at what categories resonate most with customers.\n",
        "\n",
        "These popular products can act as gateway items for bundling strategies or upselling opportunities.\n",
        "\n",
        "Consistently high-selling items likely contribute a large portion of revenue and customer retention.\n",
        "\n",
        "These insights not only reveal customer preferences but also highlight the backbone of the product portfolio that drives regular sales."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "\n",
        "Knowing the bestsellers helps optimize inventory levels, preventing both overstock and stockouts.\n",
        "\n",
        "Allows marketers to focus promotions, ads, or cross-sell campaigns around proven products.\n",
        "\n",
        "Supports development of personalized recommendations, increasing cart size and conversion rates.\n",
        "\n",
        "Top products can be used as entry points for acquiring new customers or incentivizing repeat purchases.\n",
        "\n",
        "Negative Growth Risks:\n",
        "\n",
        "Overdependence on a few top-selling items can be risky. If demand shifts or supply issues arise, the business may suffer a sales dip.\n",
        "\n",
        "Focusing only on top sellers may cause neglect of long-tail products, some of which may have high margins or niche loyal audiences.\n",
        "\n",
        "If the top-selling products have low profitability margins, scaling them aggressively without cost controls could reduce net revenue."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure the 'InvoiceDate' column is datetime\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "\n",
        "# Create 'MonthYear' column\n",
        "df['MonthYear'] = df['InvoiceDate'].dt.to_period('M')\n",
        "\n",
        "# Calculate monthly revenue\n",
        "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
        "monthly_revenue = df.groupby('MonthYear')['TotalPrice'].sum().reset_index()\n",
        "monthly_revenue['MonthYear'] = monthly_revenue['MonthYear'].astype(str)\n",
        "\n",
        "# Plot the revenue trend\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(monthly_revenue['MonthYear'], monthly_revenue['TotalPrice'], marker='o', linestyle='-', color='teal')\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('Monthly Revenue Trend')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Total Revenue')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line chart was chosen because it is ideal for displaying trends over time—especially when working with continuous data like monthly revenue. This chart provides a clear visualization of patterns, fluctuations, and potential seasonality in sales performance. It helps identify whether revenue is growing, declining, or staying consistent, which is crucial for any e-commerce business looking to scale or optimize operations."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the revenue trend line chart, we observed the following insights:\n",
        "\n",
        "There are clear spikes in revenue during certain months, possibly corresponding to holiday seasons, promotional campaigns, or year-end buying behavior.\n",
        "\n",
        "Some months show flat or declining revenue, indicating low customer activity, possibly due to off-season shopping periods or operational issues.\n",
        "\n",
        "The overall trajectory can indicate growth trends, business slowdowns, or the impact of strategic changes like marketing efforts or product additions."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "\n",
        "Enables data-driven forecasting to prepare for high-demand months and manage inventory effectively.\n",
        "\n",
        "Allows businesses to time their promotions and marketing campaigns to coincide with peak seasons.\n",
        "\n",
        "Helps in setting monthly sales targets based on historical data and trends.\n",
        "\n",
        "Informs cash flow management, ensuring the business is prepared for low-revenue months.\n",
        "\n",
        "Negative Growth Indicators:\n",
        "\n",
        "A visible drop in revenue during certain months might indicate customer churn, poor customer experience, seasonal disinterest, or operational bottlenecks.\n",
        "\n",
        "If the trend reveals a declining slope over multiple months, it could reflect market saturation, increased competition, or ineffective pricing strategies.\n",
        "\n",
        "Without action on these insights, such patterns could negatively impact long-term sustainability."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute revenue by country (excluding United Kingdom)\n",
        "country_revenue = df[df['Country'] != 'United Kingdom'].groupby('Country')['TotalPrice'].sum()\n",
        "\n",
        "# Get top 5 revenue-generating countries\n",
        "top_countries = country_revenue.sort_values(ascending=False).head(5)\n",
        "\n",
        "# Pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(top_countries, labels=top_countries.index, autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired.colors)\n",
        "plt.title('Top 5 Countries by Revenue (Excluding UK)')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart was chosen because it is ideal for visualizing proportional data—in this case, the share of total revenue contributed by each of the top-performing countries outside the United Kingdom. Since the UK dominates the dataset, excluding it allows us to focus on other international markets and understand their relative importance. Pie charts are especially effective when comparing parts of a whole, making it visually simple to communicate market concentration across countries."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the pie chart, we gain several insights:\n",
        "\n",
        "A few countries contribute a significant share of the total non-UK revenue, indicating key international markets.\n",
        "\n",
        "Countries like Netherlands, Germany, France, Ireland, and Spain (or other top 5, depending on the actual data) emerge as reliable revenue sources, suggesting successful penetration in these regions.\n",
        "\n",
        "The distribution of revenue is skewed, meaning some countries dominate while others contribute marginally, showing an opportunity to diversify."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "\n",
        "Identifying top international markets helps in targeted marketing, local partnerships, and localized product offerings.\n",
        "\n",
        "Encourages investment in countries showing high engagement and conversion rates.\n",
        "\n",
        "Provides a base to assess and scale globally beyond the domestic market (UK).\n",
        "\n",
        "Helps diversify revenue streams, reducing dependence on a single country.\n",
        "\n",
        "Negative Growth Indicators:\n",
        "\n",
        "If only a few countries dominate and others show negligible revenue, it reveals limited global penetration, which could hinder scalability.\n",
        "\n",
        "Heavy reliance on one or two foreign markets introduces risk exposure (e.g., due to regulatory, economic, or geopolitical changes in those countries).\n",
        "\n",
        "If certain high-potential markets are underperforming, it may indicate poor marketing localization, shipping issues, or mismatch between product and market demand."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute the correlation matrix (only numerical columns)\n",
        "corr_matrix = df[['Quantity', 'UnitPrice', 'TotalPrice']].corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title('Correlation Heatmap of Numerical Features')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation heatmap is a powerful tool to quickly identify linear relationships among numerical features in a dataset. It was chosen to:\n",
        "\n",
        "Understand how variables like Quantity, UnitPrice, and TotalPrice interact with one another.\n",
        "\n",
        "Detect any strong positive or negative correlations, which can help in modeling, pricing strategy, and fraud detection.\n",
        "\n",
        "Visualize correlations in a clean, interpretable matrix format that aids in decision-making.\n",
        "\n"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the heatmap, the following insights are generally observed:\n",
        "\n",
        "There is a strong positive correlation between Quantity and TotalPrice, as expected—buying more units usually increases the total transaction value.\n",
        "\n",
        "A moderate to low correlation is seen between UnitPrice and TotalPrice, indicating that price alone is not the only driver of revenue.\n",
        "\n",
        "There may be low or even negative correlation between UnitPrice and Quantity, possibly implying that customers tend to buy higher quantities of lower-priced items."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "\n",
        "Understanding these correlations enables better pricing strategies and inventory planning. For example, bundling high-quantity items at lower prices could maximize revenue.\n",
        "\n",
        "Insights into which variables influence revenue help refine recommendation systems and customer segmentation models.\n",
        "\n",
        "Weak or zero correlation between some variables might highlight independent behavior, which is useful for developing multi-variable regression models in forecasting.\n",
        "\n",
        "Potential Negative Indicators:\n",
        "\n",
        "If TotalPrice is too heavily dependent on Quantity and not much on UnitPrice, the business may be undervaluing premium pricing opportunities.\n",
        "\n",
        "A negative correlation between UnitPrice and Quantity may indicate price sensitivity among customers, which could be a risk if prices are raised without added value.\n",
        "\n",
        "If variables are too weakly correlated, it may reveal data quality issues or ineffective pricing/discount strategies.\n",
        "\n"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set figure size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Scatter plot\n",
        "plt.scatter(df['UnitPrice'], df['Quantity'], alpha=0.5, c='green', edgecolors='w')\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('Scatter Plot of Unit Price vs Quantity')\n",
        "plt.xlabel('Unit Price')\n",
        "plt.ylabel('Quantity')\n",
        "plt.xlim(0, 100)  # limit x-axis to remove extreme outliers\n",
        "plt.ylim(0, 500)  # limit y-axis to remove extreme outliers\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatter plot was selected to explore the relationship between Unit Price and Quantity across all transactions. This type of chart is ideal because:\n",
        "\n",
        "It highlights trends, clusters, and outliers between two continuous variables.\n",
        "\n",
        "It helps identify whether low-priced products are sold in large volumes or if high-priced products are sold in small quantities.\n",
        "\n",
        "It is effective for uncovering pricing anomalies or unusual purchasing behaviors that may not be visible in aggregated data."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the scatter plot, several key insights can be observed:\n",
        "\n",
        "Most purchases are concentrated in the low-price, high-quantity range, suggesting that customers tend to buy more when prices are low.\n",
        "\n",
        "High-priced items are generally purchased in lower quantities, which is expected due to cost sensitivity.\n",
        "\n",
        "A few extreme outliers (very high quantities or prices) might indicate:\n",
        "\n",
        "Bulk purchases (e.g., for events or resellers).\n",
        "\n",
        "Potential data entry errors (e.g., incorrect quantity or price input).\n",
        "\n",
        "Rare or luxury product sales."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "\n",
        "The insights validate that volume sales are driven by lower prices, which can help in designing promotional bundles or bulk discount offers.\n",
        "\n",
        "It provides evidence to segment products by price sensitivity, aiding in better recommendation systems and inventory allocation.\n",
        "\n",
        "Identifying outliers enables investigation into data quality issues or unique customer needs (e.g., B2B buyers), opening up new business channels.\n",
        "\n",
        "Potential Negative Growth Indicators:\n",
        "\n",
        "If too much revenue depends on low-margin, high-volume items, the business may struggle with profitability.\n",
        "\n",
        "High reliance on bulk purchases by few customers poses a concentration risk—losing those customers could significantly affect sales.\n",
        "\n",
        "Unchecked outliers might indicate fraudulent transactions or system glitches, which can distort analytics and lead to poor decisions."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Group by product description and sum quantities\n",
        "top_products = df.groupby('Description')['Quantity'].sum().sort_values(ascending=False).head(10)\n",
        "\n",
        "# Plot the bar chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_products.plot(kind='bar', color='skyblue', edgecolor='black')\n",
        "\n",
        "# Add chart titles and labels\n",
        "plt.title('Top 10 Most Purchased Products by Quantity')\n",
        "plt.ylabel('Total Quantity Sold')\n",
        "plt.xlabel('Product Description')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "# Show the chart\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bar chart was chosen because it is one of the most effective ways to compare discrete categories—in this case, different products—based on a measurable value (total quantity sold). This chart type allows for:\n",
        "\n",
        "Easy comparison of top-selling products.\n",
        "\n",
        "Clear visualization of which items contribute most to sales volume.\n",
        "\n",
        "Quick identification of product popularity for strategic inventory planning.\n",
        "\n"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key insights from this chart include:\n",
        "\n",
        "The top 10 products contribute significantly to the total quantity sold, indicating strong customer preference.\n",
        "\n",
        "There is a steep drop in quantity after the top few items, suggesting a long tail of less frequently sold products.\n",
        "\n",
        "The best-selling products are likely inexpensive, fast-moving items, which are ideal for volume-driven business strategies."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Positive Business Impact:\n",
        "\n",
        "Focusing on high-volume products can optimize inventory turnover and reduce holding costs.\n",
        "\n",
        "The data supports better product placement, promotions, and bundling strategies by leveraging popular items to boost basket size.\n",
        "\n",
        "This can guide supply chain and procurement planning, ensuring availability of top sellers and minimizing stockouts.\n",
        "\n",
        "Potential Negative Growth Indicators:\n",
        "\n",
        "Overdependence on a few best-selling products could be risky if market preferences change or if supply chain disruptions occur.\n",
        "\n",
        "Other products may be neglected, resulting in wasted shelf space or missed niche opportunities.\n",
        "\n",
        "If top-selling products have low profit margins, relying on volume alone may not be financially sustainable without cost control."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure InvoiceDate is in datetime format\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "\n",
        "# Create a new column with Year-Month\n",
        "df['YearMonth'] = df['InvoiceDate'].dt.to_period('M')\n",
        "\n",
        "# Calculate monthly sales (Quantity * UnitPrice)\n",
        "df['Sales'] = df['Quantity'] * df['UnitPrice']\n",
        "monthly_sales = df.groupby('YearMonth')['Sales'].sum()\n",
        "\n",
        "# Plotting the monthly sales trend\n",
        "plt.figure(figsize=(12, 6))\n",
        "monthly_sales.plot(marker='o', linestyle='-', color='teal')\n",
        "\n",
        "# Chart labeling\n",
        "plt.title('Monthly Sales Trend Over Time')\n",
        "plt.xlabel('Year-Month')\n",
        "plt.ylabel('Total Sales')\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line chart was selected because it effectively visualizes time series data, showing how total sales evolve over months. It’s especially useful for:\n",
        "\n",
        "Identifying seasonal trends, patterns, or cyclic behavior in customer purchases.\n",
        "\n",
        "Highlighting sales growth or decline over time.\n",
        "\n",
        "Helping businesses understand how specific months perform relative to others, which is essential for forecasting and planning.\n",
        "\n"
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key insights from the chart:\n",
        "\n",
        "Clear seasonality or spikes may be visible during certain months (e.g., holiday seasons, year-end).\n",
        "\n",
        "There may be sales drops in specific periods, which could be linked to external factors like holidays, supply issues, or reduced demand.\n",
        "\n",
        "A general trend (upward or downward) can indicate business performance trajectory—growth, stagnation, or decline."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "\n",
        "Businesses can use this chart for demand forecasting and inventory planning. Knowing peak sales months helps prevent understocking or overstocking.\n",
        "\n",
        "Marketing efforts can be aligned with high-demand periods, leading to improved ROI.\n",
        "\n",
        "Budgeting and staffing decisions can be optimized based on expected monthly demand.\n",
        "\n",
        "Potential Negative Growth Indicators:\n",
        "\n",
        "Sudden or consistent decline in monthly sales may signal customer churn, operational issues, or competitive loss.\n",
        "\n",
        "Months with no or very low sales may indicate systematic problems such as poor marketing, seasonal dependence, or product relevance decline.\n",
        "\n",
        "Ignoring insights from seasonal trends can lead to missed opportunities or inefficiencies in campaign planning and resource allocation."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Filter to remove extreme outliers and missing values\n",
        "filtered_df = df[(df['UnitPrice'] > 0) & (df['UnitPrice'] < 1000) & df['Country'].notnull()]\n",
        "\n",
        "# Plotting boxplot\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.boxplot(data=filtered_df, x='Country', y='UnitPrice')\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Distribution of Unit Price by Country')\n",
        "plt.xlabel('Country')\n",
        "plt.ylabel('Unit Price')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The boxplot was selected because it is ideal for comparing the distribution of unit prices across multiple countries. It provides a visual summary of:\n",
        "\n",
        "Median prices, interquartile ranges (IQR), and variability across countries.\n",
        "\n",
        "Outliers that could indicate data issues or unique high-priced items.\n",
        "\n",
        "Country-wise pricing patterns for market segmentation and pricing strategies."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights from the boxplot:\n",
        "\n",
        "Some countries show a narrow price range, indicating consistent pricing strategies (e.g., standardized product pricing).\n",
        "\n",
        "Other countries display a wide range of prices or many outliers, suggesting:\n",
        "\n",
        "Presence of premium or luxury products.\n",
        "\n",
        "Possible data entry errors or exceptional high-value items.\n",
        "\n",
        "For example, the UK (usually dominant in this dataset) may show higher variation due to being the primary market."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "\n",
        "Helps in setting optimal prices tailored to local markets.\n",
        "\n",
        "Identifies pricing inconsistencies that can be corrected for better customer trust.\n",
        "\n",
        "Supports international market analysis by comparing how similar products are priced in different countries.\n",
        "\n",
        "Detecting outliers can help clean the data, ensuring accurate reporting and better decision-making.\n",
        "\n",
        "Potential Negative Growth Indicators:\n",
        "\n",
        "Unjustified price variance across countries might lead to customer dissatisfaction or loss of trust, especially in a globally connected e-commerce platform.\n",
        "\n",
        "Outliers may reveal data quality issues such as mispriced items, which could lead to customer disputes, returns, or loss of revenue.\n",
        "\n",
        "Lack of a clear pricing pattern might signal inefficient pricing strategy or poor segmentation, which could hinder profitability.\n",
        "\n"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Select numerical columns for correlation matrix\n",
        "numeric_df = df[['Quantity', 'UnitPrice', 'Sales']]\n",
        "\n",
        "# Compute the correlation matrix\n",
        "correlation_matrix = numeric_df.corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title('Correlation Heatmap of Numerical Features')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation heatmap is chosen because it visually highlights the strength and direction of linear relationships between numerical variables in the dataset. It's particularly useful in:\n",
        "\n",
        "Identifying positive or negative correlations among key business metrics like Quantity, UnitPrice, and Sales.\n",
        "\n",
        "Detecting variables that may influence each other (e.g., higher quantity purchased may increase total sales).\n",
        "\n",
        "Helping in feature selection for machine learning models by identifying redundancy or strong predictors."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the heatmap of the correlation matrix (typically among Quantity, UnitPrice, and Sales):\n",
        "\n",
        "There is a strong positive correlation between Quantity and Sales — indicating that higher quantities ordered directly contribute to increased sales revenue.\n",
        "\n",
        "The correlation between UnitPrice and Sales may vary:\n",
        "\n",
        "If it’s low or negative, it could imply that higher prices don’t necessarily mean higher revenue, possibly due to lower sales volume for expensive items.\n",
        "\n",
        "Quantity and UnitPrice may have a slight negative correlation, suggesting that higher quantity orders are often for lower-priced items, which is common in bulk purchases or wholesale transactions."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select relevant numeric columns and drop NaNs if any\n",
        "pairplot_df = df[['Quantity', 'UnitPrice', 'Sales']].dropna()\n",
        "\n",
        "# Generate the pairplot\n",
        "sns.pairplot(pairplot_df, diag_kind='kde', corner=True)\n",
        "plt.suptitle('Pair Plot of Quantity, UnitPrice, and Sales', y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pair plot is chosen because it allows us to visualize relationships between multiple numerical variables simultaneously. It is especially useful for:\n",
        "\n",
        "Detecting linear or non-linear relationships between variables like Quantity, UnitPrice, and Sales.\n",
        "\n",
        "Spotting clusters, trends, or outliers across different feature combinations.\n",
        "\n",
        "Providing an intuitive, visual summary of how variables interact — helpful before applying machine learning models or clustering algorithms."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the pair plot:\n",
        "\n",
        "Quantity vs. Sales shows a strong positive linear relationship, confirming that sales are largely quantity-driven.\n",
        "\n",
        "UnitPrice vs. Quantity reveals a negative or scattered trend, suggesting that cheaper items are typically purchased in larger quantities (possibly wholesale or discount purchases).\n",
        "\n",
        "UnitPrice vs. Sales may not show a strong correlation, implying that high-priced products don't necessarily bring in more sales, unless purchased in quantity.\n",
        "\n",
        "The distributions (diagonal plots) show:\n",
        "\n",
        "Quantity has a right-skewed distribution, with most transactions involving small quantities.\n",
        "\n",
        "UnitPrice is also right-skewed, meaning most items are low-cost, with a few expensive ones.\n",
        "\n",
        "Sales follows a similar skew, with many small-value transactions and a few high-value ones.\n",
        "\n"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis 1 (H1):\n",
        "Customers purchase significantly more units of low-priced products than high-priced ones.\n",
        "\n",
        "Null Hypothesis (H0): There is no significant difference in quantity purchased between low-priced and high-priced products.\n",
        "\n",
        "Alternate Hypothesis (H1): There is a significant difference in quantity purchased between low-priced and high-priced products.\n",
        "\n",
        "Hypothesis 2 (H2):\n",
        "The average sales per transaction is the same for domestic (UK) and international customers.\n",
        "\n",
        "Null Hypothesis (H0): There is no difference in mean sales per transaction between UK and international customers.\n",
        "\n",
        "Alternate Hypothesis (H1): There is a difference in mean sales per transaction between UK and international customers.\n",
        "\n",
        "Hypothesis 3 (H3):\n",
        "The average unit price differs significantly between top 5 most active countries (excluding UK).\n",
        "\n",
        "Null Hypothesis (H0): The average unit price is equal across top 5 countries.\n",
        "\n",
        "Alternate Hypothesis (H1): At least one country differs significantly in average unit price.\n",
        "\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀):\n",
        "There is no significant difference in the average quantity purchased between low-priced and high-priced products. Any variation observed in quantities bought is due to random chance and not due to price differences.\n",
        "\n",
        "Alternate Hypothesis (H₁):\n",
        "There is a significant difference in the average quantity purchased between low-priced and high-priced products. This suggests that pricing affects customer purchase volume and plays a key role in determining how much of a product is bought.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Reload dataset (if needed)\n",
        "df = pd.read_csv(\"online_retail.csv\", encoding='ISO-8859-1')\n",
        "\n",
        "# Drop missing values in essential columns\n",
        "df = df.dropna(subset=['Quantity', 'UnitPrice'])\n",
        "\n",
        "# Create low and high price groups (threshold = median price)\n",
        "median_price = df['UnitPrice'].median()\n",
        "low_price_group = df[df['UnitPrice'] <= median_price]['Quantity']\n",
        "high_price_group = df[df['UnitPrice'] > median_price]['Quantity']\n",
        "\n",
        "# Perform independent t-test\n",
        "t_stat, p_value = ttest_ind(low_price_group, high_price_group, equal_var=False)\n",
        "\n",
        "print(\"T-statistic:\", t_stat)\n",
        "print(\"P-value:\", p_value)"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test Hypothesis 1, an Independent Samples t-test was conducted to evaluate whether there is a statistically significant difference in the average quantity purchased between low-priced and high-priced products. This test was chosen because it is appropriate for comparing the means of two independent groups where the dependent variable (in this case, the quantity of products purchased) is continuous, and the grouping variable (product price) is categorical after dividing it based on the median value. The independent t-test assumes that the two groups are mutually exclusive and that the samples are randomly drawn, which aligns with our dataset conditions. By applying this test, we obtained a p-value, which helps in determining whether to accept or reject the null hypothesis. A low p-value would suggest a significant difference between the two price groups, indicating that product pricing does have a measurable impact on the volume of items purchased. This insight can be vital for pricing strategies in e-commerce platforms.\n"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Independent Samples t-test was chosen because the objective was to compare the average quantity of items purchased between two independent groups — products categorized as low-priced and high-priced, based on their median unit price. This test is ideal when you want to assess whether the means of a continuous variable (like quantity) differ significantly across two unrelated groups.\n",
        "\n",
        "In our dataset, each transaction is an independent observation, and customers purchasing low-priced items are not necessarily the same as those purchasing high-priced ones. The t-test accounts for differences in sample sizes and variance between the two groups, making it a robust choice for analyzing real-world business data like this. Moreover, the sample size in the dataset is large enough for the t-test to remain valid even if the underlying data isn't perfectly normally distributed."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Hypothesis 2, the research investigates whether customers from different countries purchase significantly different quantities of products. The null hypothesis (H₀) assumes that there is no significant difference in the average quantity purchased among customers across various countries. In other words, the purchasing behavior, in terms of quantity, is consistent regardless of a customer’s location. Conversely, the alternative hypothesis (H₁) posits that there is a significant difference in the average quantity purchased between customers of at least two different countries. This suggests that a customer's geographical location may influence how much they purchase. This hypothesis is essential for understanding regional demand patterns and can help e-commerce businesses optimize their inventory, pricing, and marketing strategies for specific countries. Detecting such differences could lead to more personalized and effective business decisions tailored to each market.\n"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"online_retail.csv\", encoding='ISO-8859-1')"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Hypothesis 2, I used the One-Way ANOVA (Analysis of Variance) statistical test to obtain the p-value.\n",
        "\n",
        "This test is specifically designed to compare the means of a numerical variable across more than two independent groups—in this case, the average quantity of products purchased across different countries. The goal was to determine whether the observed differences in purchase quantities among customers from various countries are statistically significant or could have occurred by random chance.\n",
        "\n",
        "The ANOVA test is appropriate here because:\n",
        "\n",
        "The independent variable (Country) is categorical with multiple groups.\n",
        "\n",
        "The dependent variable (Quantity) is continuous and numerical.\n",
        "\n",
        "We are comparing more than two groups (multiple countries).\n",
        "\n",
        "The resulting very low p-value indicates strong evidence against the null hypothesis, confirming that country-wise purchasing behavior significantly differs."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Hypothesis 2, the One-Way ANOVA (Analysis of Variance) test was chosen as the appropriate statistical method to obtain the p-value. The primary reason for this choice lies in the nature of the variables involved and the objective of the analysis. In this case, the research aimed to determine whether the average quantity of products purchased by customers significantly differs across different countries. This scenario involves comparing the mean values of a numerical variable (Quantity) across multiple categorical groups (Countries).\n",
        "\n",
        "A one-way ANOVA is specifically designed for such situations. It evaluates whether there are any statistically significant differences between the means of three or more independent (unrelated) groups. While a t-test is suitable for comparing the means of two groups, it becomes less reliable and increases the chance of Type I errors when extended to multiple group comparisons. ANOVA, on the other hand, maintains statistical accuracy and reliability by comparing all group variances simultaneously.\n",
        "\n",
        "Additionally, the dataset used contains a large number of observations, which meets the condition for ANOVA regarding sample size. The groups (countries) are independent of each other, and although perfect normality and equal variances (homoscedasticity) are ideal conditions for ANOVA, the test is robust to moderate violations of these assumptions when sample sizes are large. Therefore, given the structure of the data and the analysis goal, One-Way ANOVA was the most statistically sound and efficient method for testing the hypothesis."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀):\n",
        "There is no significant difference in the average quantity of items purchased across different times of the day (morning, afternoon, and evening).\n",
        "In other words, time of day does not influence purchasing quantity.\n",
        "\n",
        "Alternate Hypothesis (H₁):\n",
        "There is a significant difference in the average quantity of items purchased across different times of the day.\n",
        "That is, time of day does influence purchasing quantity."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"online_retail.csv\", encoding='ISO-8859-1')\n",
        "\n",
        "# Drop missing values from necessary columns\n",
        "df = df.dropna(subset=['Quantity', 'InvoiceDate'])\n",
        "\n",
        "# Convert InvoiceDate to datetime\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "\n",
        "# Define time of day based on the hour\n",
        "def get_time_of_day(hour):\n",
        "    if 6 <= hour < 12:\n",
        "        return 'Morning'\n",
        "    elif 12 <= hour < 18:\n",
        "        return 'Afternoon'\n",
        "    else:\n",
        "        return 'Evening'\n",
        "\n",
        "df['TimeOfDay'] = df['InvoiceDate'].dt.hour.apply(get_time_of_day)\n",
        "\n",
        "# Separate quantities for each time slot\n",
        "morning = df[df['TimeOfDay'] == 'Morning']['Quantity']\n",
        "afternoon = df[df['TimeOfDay'] == 'Afternoon']['Quantity']\n",
        "evening = df[df['TimeOfDay'] == 'Evening']['Quantity']\n",
        "\n",
        "# Perform ANOVA test\n",
        "anova_result = f_oneway(morning, afternoon, evening)\n",
        "print(\"F-statistic:\", anova_result.statistic)\n",
        "print(\"P-value:\", anova_result.pvalue)\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The One-Way ANOVA test is appropriate when you want to compare the means of a continuous variable (like Quantity of products purchased) across more than two independent groups (in this case: Morning, Afternoon, and Evening time periods).\n",
        "\n",
        "Null Hypothesis (H₀): There is no significant difference in the average quantity of purchases across different times of day.\n",
        "\n",
        "Alternative Hypothesis (H₁): There is a significant difference in the average quantity of purchases across different times of day.\n",
        "\n"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The One-Way ANOVA (Analysis of Variance) test was chosen for Hypothesis 3 because it is specifically designed to compare the means of a continuous variable across three or more independent groups. In this case, the groups are time segments of the day—Morning, Afternoon, and Evening—and the continuous variable being measured is the Quantity of items purchased.\n",
        "\n",
        "Unlike a t-test (which compares means between only two groups), ANOVA is suitable when there are more than two groups, and we want to determine whether at least one group’s mean is statistically significantly different from the others. Additionally, ANOVA helps avoid the increased risk of Type I error that comes with performing multiple pairwise t-tests."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"online_retail.csv\", encoding='ISO-8859-1')\n",
        "\n",
        "# Show total missing values per column\n",
        "print(\"Missing Values Per Column:\\n\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Drop rows where CustomerID is missing, as it's important for customer-level analysis\n",
        "df = df.dropna(subset=['CustomerID'])\n",
        "\n",
        "# Impute missing values in 'Description' with a placeholder\n",
        "df['Description'] = df['Description'].fillna('No Description')\n",
        "\n",
        "# If any other numerical columns have missing values, fill them with median (if applicable)\n",
        "if df['Quantity'].isnull().sum() > 0:\n",
        "    df['Quantity'] = df['Quantity'].fillna(df['Quantity'].median())\n",
        "\n",
        "if df['UnitPrice'].isnull().sum() > 0:\n",
        "    df['UnitPrice'] = df['UnitPrice'].fillna(df['UnitPrice'].median())\n",
        "\n",
        "# Verify no missing values remain\n",
        "print(\"\\nMissing Values After Imputation:\\n\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In handling the missing values in the Online Retail dataset, a combination of imputation techniques was applied based on the nature and importance of each variable. For the CustomerID column, rows with missing values were dropped entirely since this column is crucial for customer-level analyses such as segmentation and personalized recommendations. Without a valid CustomerID, it would be impossible to associate transactions with specific customers, which would compromise the integrity of any insights derived from them.\n",
        "\n",
        "For the Description column, which contains text values for product names, missing entries were imputed using a constant value—specifically, the string “No Description.” This approach preserves the data for further analysis without assuming incorrect product information, and avoids discarding useful transaction data tied to identifiable customers.\n",
        "\n",
        "If any missing values were found in numeric fields such as Quantity or UnitPrice, median imputation was used. The median is preferred in such cases because it is robust to outliers and better represents the central tendency of skewed distributions, which are common in real-world transactional datasets. These techniques were selected to ensure the cleaned dataset maintained its analytical value while minimizing the risk of introducing bias or noise through imputation."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('online_retail.csv', encoding='ISO-8859-1')\n",
        "\n",
        "# Remove rows with missing CustomerID and non-positive Quantity or UnitPrice\n",
        "df = df.dropna(subset=['CustomerID'])\n",
        "df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]\n",
        "\n",
        "# Function to remove outliers using IQR method\n",
        "def remove_outliers_iqr(data, column):\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    return data[(data[column] >= lower) & (data[column] <= upper)]\n",
        "\n",
        "# Before removing outliers\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(x=df['Quantity'])\n",
        "plt.title('Before Outlier Removal - Quantity')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(x=df['UnitPrice'])\n",
        "plt.title('Before Outlier Removal - UnitPrice')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Apply outlier removal\n",
        "df_clean = remove_outliers_iqr(df, 'Quantity')\n",
        "df_clean = remove_outliers_iqr(df_clean, 'UnitPrice')\n",
        "\n",
        "# After removing outliers\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(x=df_clean['Quantity'])\n",
        "plt.title('After Outlier Removal - Quantity')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(x=df_clean['UnitPrice'])\n",
        "plt.title('After Outlier Removal - UnitPrice')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Optional: Show how many rows were removed\n",
        "removed_rows = len(df) - len(df_clean)\n",
        "print(f\"Number of rows removed due to outliers: {removed_rows}\")"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To ensure the quality and reliability of the dataset, outliers were identified and handled using the Interquartile Range (IQR) method. This technique is widely used in exploratory data analysis due to its simplicity and effectiveness in identifying extreme values that may skew the results. The IQR method defines outliers as data points that fall below Q1 − 1.5×IQR or above Q3 + 1.5×IQR, where Q1 and Q3 represent the 25th and 75th percentiles, respectively. This was particularly important for the Quantity and UnitPrice fields, as these variables exhibited extreme values that could distort statistical analyses and machine learning models. By removing these outliers rather than capping them, we preserved the integrity of the remaining data while eliminating noise caused by unusually high or low transaction values. This method is non-parametric and does not assume a specific distribution, making it suitable for the skewed nature of transaction data often seen in retail datasets."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Make a copy of the dataset to work with\n",
        "df_encoded = df.copy()\n",
        "\n",
        "# Check categorical columns\n",
        "categorical_columns = df_encoded.select_dtypes(include=['object']).columns\n",
        "print(\"Categorical columns:\", categorical_columns)\n",
        "\n",
        "# Example: Label Encoding for 'Country'\n",
        "label_encoder = LabelEncoder()\n",
        "df_encoded['Country_encoded'] = label_encoder.fit_transform(df_encoded['Country'])\n",
        "\n",
        "# You may also use One-Hot Encoding for nominal categorical data\n",
        "df_encoded = pd.get_dummies(df_encoded, columns=['Country'], drop_first=True)\n",
        "\n",
        "# Drop original object columns if needed\n",
        "# df_encoded = df_encoded.drop(['InvoiceNo', 'StockCode', 'Description'], axis=1)\n",
        "\n",
        "# Display encoded DataFrame\n",
        "df_encoded.head()\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the preprocessing of the Online Retail dataset, categorical columns such as Country were encoded to make them suitable for machine learning models. For this, two common encoding techniques were applied: Label Encoding and One-Hot Encoding. Label Encoding was used on the Country column when preparing data for tasks like clustering, where efficiency and compact representation are important. It assigns a unique numerical value to each category, making it memory-efficient, especially for high-cardinality columns. On the other hand, One-Hot Encoding was also applied to the same column when preparing the dataset for supervised learning models such as regression or classification. This technique avoids implying any ordinal relationship among categories by converting them into binary columns, which can improve model performance and interpretability. Identifier columns like InvoiceNo and StockCode were excluded from encoding since they do not represent categorical variables with meaningful influence on the outcome and could introduce noise if encoded. Overall, the encoding strategy was chosen based on the data type, the cardinality of the variables, and the intended machine learning task."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "import contractions\n",
        "\n",
        "# Example text or column\n",
        "text = \"I can't go because it's not allowed.\"\n",
        "\n",
        "# Expand the contractions in a single sentence\n",
        "expanded_text = contractions.fix(text)\n",
        "print(\"Expanded Text:\", expanded_text)\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "text = \"This is A Sample TEXT.\"\n",
        "lower_text = text.lower()\n",
        "print(\"Lowercased Text:\", lower_text)\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import string\n",
        "\n",
        "# Sample text\n",
        "text = \"Hello! This is a sample, with punctuation marks: isn't it?\"\n",
        "\n",
        "# Remove punctuation\n",
        "no_punct_text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "print(\"Text without punctuation:\", no_punct_text)\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs\n",
        "import re\n",
        "\n",
        "# Sample text\n",
        "text = \"Visit https://example.com for 50% off on shoes123 and best4you deals.\"\n",
        "\n",
        "# Step 1: Remove URLs\n",
        "text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "# Step 2: Remove words that contain digits (like shoes123, best4you)\n",
        "text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "\n",
        "# Step 3: Remove extra spaces\n",
        "text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove words and digits contain digits\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Sample DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Review_Text': [\n",
        "        'Visit https://example.com for 50% off on shoes123!',\n",
        "        'Check www.deal4u.com now or call me at 123-456-7890.'\n",
        "    ]\n",
        "})\n",
        "\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
        "        text = re.sub(r'\\w*\\d\\w*', '', text)               # Remove words with digits\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()           # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "# Apply the function to the DataFrame\n",
        "df['Cleaned_Review'] = df['Review_Text'].apply(clean_text)\n",
        "\n",
        "print(df[['Review_Text', 'Cleaned_Review']])\n"
      ],
      "metadata": {
        "id": "JZPv2HA0y46V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing Stopwords\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords once (if not already)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load stopwords list\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Sample DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Text': [\n",
        "        \"  This is a sample sentence with some stopwords.   \",\n",
        "        \"Another     example     with   excessive   spaces and stopwords like the, is, at.\"\n",
        "    ]\n",
        "})\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "def remove_stopwords_and_whitespace(text):\n",
        "    if isinstance(text, str):\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        # Tokenize and remove stopwords\n",
        "        words = text.split()\n",
        "        filtered = [word for word in words if word.lower() not in stop_words]\n",
        "        return ' '.join(filtered)\n",
        "    return text\n",
        "\n",
        "# Apply to DataFrame\n",
        "df['Cleaned_Text'] = df['Text'].apply(remove_stopwords_and_whitespace)\n",
        "\n",
        "# Show result\n",
        "print(df[['Text', 'Cleaned_Text']])"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "# Install the transformers library\n",
        "!pip install transformers\n",
        "\n",
        "# Import necessary libraries\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the paraphrasing pipeline using T5 model\n",
        "paraphraser = pipeline(\"text2text-generation\", model=\"Vamsi/T5_Paraphrase_Paws\")\n",
        "\n",
        "# Sample input text\n",
        "text = \"Machine learning is a method of data analysis that automates analytical model building.\"\n",
        "\n",
        "# Generate rephrased (paraphrased) versions\n",
        "rephrased = paraphraser(f\"paraphrase: {text} </s>\", max_length=256, num_return_sequences=3, do_sample=True)\n",
        "\n",
        "# Display the paraphrased sentences\n",
        "for i, result in enumerate(rephrased, 1):\n",
        "    print(f\"Rephrased {i}: {result['generated_text']}\")"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the 'punkt' and 'punkt_tab' tokenizer if you haven't already\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Make sure the 'Cleaned_Text' column exists and handle potential non-string values\n",
        "if 'Cleaned_Text' in df.columns:\n",
        "    df['Cleaned_Text'] = df['Cleaned_Text'].astype(str)\n",
        "    df['Tokenized_Text'] = df['Cleaned_Text'].apply(word_tokenize)\n",
        "    print(\"Tokenized DataFrame (first 5 rows):\\n\", df[['Cleaned_Text', 'Tokenized_Text']].head())\n",
        "else:\n",
        "    # Example with a sample string if the DataFrame is not set up as expected\n",
        "    sample_text = \"This is a sample sentence for tokenization.\"\n",
        "    tokens = word_tokenize(sample_text)\n",
        "    print(\"Tokenized sample text:\", tokens)"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Download the specific resource\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to get the appropriate WordNet POS tag for lemmatization\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# Apply stemming and lemmatization\n",
        "if 'Tokenized_Text' in df.columns:\n",
        "    df['Stemmed_Text'] = df['Tokenized_Text'].apply(lambda tokens: [stemmer.stem(word) for word in tokens])\n",
        "    df['Lemmatized_Text'] = df['Tokenized_Text'].apply(lambda tokens: [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens])\n",
        "    print(\"Normalized DataFrame (first 5 rows):\\n\", df[['Tokenized_Text', 'Stemmed_Text', 'Lemmatized_Text']].head())\n",
        "else:\n",
        "    # Example with sample tokens\n",
        "    sample_tokens = [\"running\", \"runs\", \"ran\", \"easily\", \"fairly\"]\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in sample_tokens]\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in sample_tokens]\n",
        "    print(\"Original tokens:\", sample_tokens)\n",
        "    print(\"Stemmatized tokens:\", stemmed_tokens)\n",
        "    print(\"Lemmatized tokens:\", lemmatized_tokens)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the text normalization process, lemmatization was used as the primary technique because it offers more accurate and meaningful results compared to stemming. Lemmatization transforms words into their base or dictionary form (known as lemmas) by considering the context and part of speech of each word. This ensures that the resulting terms are valid words that preserve the original meaning of the text. For instance, words like “running” or “better” would be converted to “run” and “good,” respectively, based on grammatical context, whereas stemming might crudely reduce them to “run” or “bet,” which may not always be appropriate or meaningful. Although stemming using algorithms like the Porter Stemmer is faster and simpler, it often leads to non-dictionary forms that reduce the interpretability of the data. Since lemmatization provides cleaner and more semantically consistent output, it was chosen to enhance the overall quality of the text data, which is crucial for tasks such as sentiment analysis, classification, or topic modeling where the preservation of meaning plays a vital role."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "import nltk\n",
        "\n",
        "# Download the 'averaged_perceptron_tagger' if not already downloaded\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Apply POS tagging\n",
        "if 'Tokenized_Text' in df.columns:\n",
        "    df['POS_Tagged_Text'] = df['Tokenized_Text'].apply(nltk.pos_tag)\n",
        "    print(\"POS Tagged DataFrame (first 5 rows):\\n\", df[['Tokenized_Text', 'POS_Tagged_Text']].head())\n",
        "else:\n",
        "    # Example with sample tokens\n",
        "    sample_tokens = [\"This\", \"is\", \"a\", \"sample\", \"sentence\", \"for\", \"POS\", \"tagging\", \".\"]\n",
        "    pos_tags = nltk.pos_tag(sample_tokens)\n",
        "    print(\"POS tagged sample tokens:\", pos_tags)"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Ensure the 'Cleaned_Text' column is available and contains string data\n",
        "if 'Cleaned_Text' in df.columns:\n",
        "    # Handle potential non-string values in the 'Cleaned_Text' column\n",
        "    df['Cleaned_Text'] = df['Cleaned_Text'].astype(str).fillna('')\n",
        "\n",
        "    # You can adjust parameters like max_features, min_df, max_df, ngram_range\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=1000) # Example: consider top 1000 terms\n",
        "\n",
        "    # Fit and transform the text data\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['Cleaned_Text'])\n",
        "\n",
        "    # Convert the TF-IDF matrix to a DataFrame (optional, for better viewing)\n",
        "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "    print(\"TF-IDF Vectorized Data Shape:\", tfidf_matrix.shape)\n",
        "    print(\"TF-IDF DataFrame (first 5 rows):\\n\", tfidf_df.head())\n",
        "else:\n",
        "    print(\"Error: 'Cleaned_Text' column not found in the DataFrame. Please ensure previous text preprocessing steps were executed.\")"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this project, the TF-IDF (Term Frequency–Inverse Document Frequency) vectorization technique was used. TF-IDF is particularly effective because it not only captures how frequently a word appears in a document (term frequency), but also considers how unique that word is across the entire dataset (inverse document frequency). This helps reduce the weight of common words (like \"the\", \"is\", \"and\") that appear in almost every document and emphasizes more meaningful, rare terms that are likely to provide better insight during analysis or classification.\n",
        "\n",
        "Compared to simple techniques like Count Vectorization, TF-IDF provides a more nuanced representation, especially useful in tasks like sentiment analysis, spam detection, or topic modeling where word relevance matters more than raw frequency."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Load your dataset - Corrected filename\n",
        "df = pd.read_csv(\"online_retail.csv\", encoding='ISO-8859-1')\n",
        "\n",
        "# Perform basic cleaning steps as done previously to ensure data consistency\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.dropna(subset=['CustomerID', 'Description'], inplace=True)\n",
        "df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "df['CustomerID'] = df['CustomerID'].astype(str)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Check correlation\n",
        "# -------------------------------\n",
        "# Select only numeric columns for correlation calculation\n",
        "numeric_df = df.select_dtypes(include=np.number)\n",
        "correlation_matrix = numeric_df.corr()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Correlation Matrix Before Feature Engineering\")\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Drop highly correlated features (correlation > 0.85)\n",
        "# -------------------------------\n",
        "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
        "high_corr_features = [column for column in upper_triangle.columns if any(upper_triangle[column].abs() > 0.85)]\n",
        "\n",
        "# Drop only if the feature exists in the DataFrame columns\n",
        "features_to_drop = [col for col in high_corr_features if col in df.columns]\n",
        "df.drop(columns=features_to_drop, inplace=True)\n",
        "\n",
        "\n",
        "print(f\"Dropped highly correlated features: {features_to_drop}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Feature Engineering\n",
        "# -------------------------------\n",
        "\n",
        "# Example: create ratio and interaction features\n",
        "# Check if features exist before creating new ones\n",
        "if {'Quantity', 'UnitPrice'}.issubset(df.columns):\n",
        "    df['Quantity_to_UnitPrice_ratio'] = df['Quantity'] / (df['UnitPrice'] + 1e-5)  # Avoid division by zero\n",
        "\n",
        "# Polynomial Features (2nd degree only on selected columns)\n",
        "# Replace with relevant numerical column names from your dataset after cleaning\n",
        "selected_cols = ['Quantity', 'UnitPrice'] # Example columns, adjust as needed\n",
        "if all(col in df.columns for col in selected_cols):\n",
        "    try:\n",
        "        poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
        "        # Ensure selected_cols are numerical and handle potential NaNs before transforming\n",
        "        poly_features = poly.fit_transform(df[selected_cols].dropna())\n",
        "        poly_feature_names = poly.get_feature_names_out(selected_cols)\n",
        "        # Create a temporary DataFrame for polynomial features and align index\n",
        "        poly_df = pd.DataFrame(poly_features, columns=poly_feature_names, index=df[selected_cols].dropna().index)\n",
        "        # Concatenate with the original DataFrame, handling missing values introduced by dropna\n",
        "        df = df.join(poly_df)\n",
        "    except ValueError as e:\n",
        "        print(f\"Could not create polynomial features. Error: {e}\")\n",
        "        print(f\"Check if selected_cols {selected_cols} are suitable for PolynomialFeatures.\")\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Correlation After Feature Engineering\n",
        "# -------------------------------\n",
        "# Select only numeric columns after feature engineering\n",
        "numeric_df_after = df.select_dtypes(include=np.number)\n",
        "correlation_matrix_after = numeric_df_after.corr()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix_after, annot=False, cmap=\"viridis\")\n",
        "plt.title(\"Correlation Matrix After Feature Engineering\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# Select only numeric columns for correlation matrix calculation\n",
        "numeric_df = df.select_dtypes(include=np.number)\n",
        "\n",
        "# 1. Remove highly correlated features\n",
        "cor_matrix = numeric_df.corr().abs()\n",
        "\n",
        "# Select upper triangle of correlation matrix\n",
        "upper = cor_matrix.where(\n",
        "    np.triu(np.ones(cor_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "# Find features with correlation greater than 0.90\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
        "df_reduced = numeric_df.drop(columns=to_drop) # Drop from numeric_df\n",
        "\n",
        "\n",
        "print(\"Dropped due to high correlation:\", to_drop)\n",
        "\n",
        "# Replace 'target_column' with your actual numerical target variable name\n",
        "target_column = 'target_column'\n",
        "\n",
        "if target_column in df_reduced.columns:\n",
        "    X = df_reduced.drop(target_column, axis=1)\n",
        "    y = df_reduced[target_column]\n",
        "\n",
        "    # Check if X and y are not empty and contain valid data for model training\n",
        "    if not X.empty and not y.empty:\n",
        "        try:\n",
        "            model = RandomForestClassifier(random_state=42)\n",
        "            model.fit(X, y)\n",
        "\n",
        "            # Show feature importance\n",
        "            feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
        "            feat_importances.sort_values(ascending=False).plot(kind='bar', figsize=(12,6))\n",
        "            plt.title(\"Feature Importances\")\n",
        "            plt.show()\n",
        "\n",
        "            # 3. Select features based on importance\n",
        "            selector = SelectFromModel(model, threshold='median')\n",
        "            selector.fit(X, y)\n",
        "            selected_features = X.columns[(selector.get_support())]\n",
        "\n",
        "            # Final dataset with selected features\n",
        "            X_selected = X[selected_features]\n",
        "            print(\"Selected Features:\\n\", selected_features.tolist())\n",
        "        except ValueError as e:\n",
        "            print(f\"Error during model training or feature selection: {e}\")\n",
        "            print(\"Please ensure your target column is suitable for classification and your features do not contain NaN or infinite values.\")\n",
        "\n",
        "    else:\n",
        "        print(\"Error: Features (X) or target (y) are empty after data reduction.\")\n",
        "else:\n",
        "    print(f\"Error: Target column '{target_column}' not found in the reduced numeric DataFrame.\")"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Correlation-Based Feature Removal\n",
        "Method: Dropped features with high correlation (correlation coefficient > 0.9).\n",
        "Why Used: Highly correlated features provide similar information to the model, leading to multicollinearity, which can distort model coefficients and reduce interpretability. Removing such features simplifies the model and improves generalization.\n",
        "\n",
        "2. Random Forest Feature Importance\n",
        "Method: Trained a Random Forest model and selected features based on importance scores.\n",
        "Why Used: Random Forest is robust and non-parametric. It automatically evaluates the contribution of each feature in reducing prediction error. This method is especially helpful when dealing with a mix of categorical and numerical data.\n",
        "\n",
        "3. Model-Based Selection (SelectFromModel)\n",
        "Method: Selected features whose importance is above a defined threshold (e.g., median).\n",
        "Why Used: This approach ensures that only the most relevant features are retained based on their impact on the target variable, reducing noise and computational cost while preserving model accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Transaction_amount\n",
        "Why important: It directly relates to the business’s revenue. Higher transaction amounts are a strong indicator of customer value and purchasing behavior.\n",
        "\n",
        "Impact: Helps in customer segmentation and targeted promotions.\n",
        "\n",
        "2. Transaction_type\n",
        "Why important: Different transaction types (like recharge, bill payments, money transfers) have varying frequencies and monetary impacts.\n",
        "\n",
        "Impact: Useful in understanding customer needs and designing service-specific campaigns.\n",
        "\n",
        "3. User_Age_Group\n",
        "Why important: Age demographics influence spending patterns and preferred services.\n",
        "\n",
        "Impact: Critical for personalized marketing and product recommendation systems.\n",
        "\n",
        "4. Region/Location\n",
        "Why important: Geographic location often correlates with economic activity and service adoption.\n",
        "\n",
        "Impact: Helps in regional performance analysis and resource allocation."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, data transformation is often necessary to improve the performance and accuracy of machine learning models. In this case, transformations were applied to ensure that the data met the assumptions of various algorithms and to enhance the model’s ability to learn from the data."
      ],
      "metadata": {
        "id": "PITn4kug4seT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Check if 'TotalPrice' column exists before transforming\n",
        "if 'TotalPrice' in df.columns:\n",
        "    df['TotalPrice_log'] = np.log1p(df['TotalPrice'])  # log1p handles log(0)\n",
        "    print(\"Transformed DataFrame (first 5 rows with new column):\\n\", df[['TotalPrice', 'TotalPrice_log']].head())\n",
        "else:\n",
        "    print(\"Error: 'TotalPrice' column not found in the DataFrame. Please ensure it was created in previous steps.\")"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Sample dataset\n",
        "data = {\n",
        "    'amount': [1000, 1500, 2000, 3000, 1200],\n",
        "    'transactions': [5, 8, 12, 7, 6],\n",
        "    'duration': [20, 25, 30, 27, 23]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Convert back to DataFrame for readability\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
        "\n",
        "# Display scaled data\n",
        "print(scaled_df)\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suitable for Normal Distributions: Many of the numerical features followed approximately normal or near-normal distributions. Standardization is most effective in such scenarios.\n",
        "\n",
        "Model Compatibility: Algorithms like Logistic Regression, Support Vector Machines (SVM), and K-Nearest Neighbors (KNN) are sensitive to feature scale. Standardization ensures that all features contribute equally to the model.\n",
        "\n",
        "Preserves Outliers Better Than Min-Max Scaling: Unlike min-max scaling, which compresses all values into a 0–1 range, standardization does not bound values, thus preserving the impact of meaningful outliers.\n",
        "\n",
        "Improved Convergence in Gradient-Based Models: It helps models like Gradient Descent-based classifiers (e.g., Logistic Regression, Neural Networks) converge faster by balancing the gradients across all dimensions."
      ],
      "metadata": {
        "id": "_rdTZThc5vJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To Reduce Multicollinearity: Highly correlated features do not provide additional information but can increase the complexity of the model and reduce interpretability. Dimensionality reduction (e.g., using PCA) can help by combining such features.\n",
        "\n",
        "To Improve Model Performance: Models trained on high-dimensional data often suffer from the curse of dimensionality, which can lead to overfitting. Reducing dimensionality simplifies the model and often improves generalization on unseen data.\n",
        "\n",
        "To Enhance Computational Efficiency: Reducing the number of features results in lower memory usage and faster computation, which is especially useful for real-time applications or large datasets.\n",
        "\n",
        "To Improve Visualization: For exploratory data analysis and clustering, techniques like PCA or t-SNE are helpful to visualize high-dimensional data in 2D or 3D space.\n",
        "\n"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'CustomerID' should not be included in PCA\n",
        "features_for_pca = df_scaled.select_dtypes(include=np.number).columns.tolist()\n",
        "cols_to_exclude_pca = ['CustomerID'] # Add any other columns to exclude from PCA\n",
        "features_for_pca = [col for col in features_for_pca if col not in cols_to_exclude_pca]\n",
        "\n",
        "# Ensure there are features to perform PCA on\n",
        "if features_for_pca:\n",
        "    X = df_scaled[features_for_pca]\n",
        "\n",
        "    # Ensure X does not contain any NaN or infinite values before scaling for PCA\n",
        "    X = X.dropna().replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "    # Check if X is empty after handling NaNs and infinities\n",
        "    if not X.empty:\n",
        "\n",
        "        # Apply PCA to retain 95% variance\n",
        "        pca = PCA(n_components=0.95)\n",
        "        X_pca = pca.fit_transform(X)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        X_pca_df = pd.DataFrame(X_pca, index=X.index, columns=[f'PC{i+1}' for i in range(X_pca.shape[1])])\n",
        "\n",
        "        # Check explained variance ratio\n",
        "        explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "        # Optional: View shape of reduced data\n",
        "        print(\"Original shape:\", X.shape)\n",
        "        print(\"Transformed shape:\", X_pca.shape)\n",
        "        print(\"Explained variance by components:\", explained_variance)\n",
        "        print(\"\\nPCA Transformed Data (first 5 rows):\\n\", X_pca_df.head())\n",
        "\n",
        "    else:\n",
        "        print(\"Error: Features for PCA are empty after handling missing/infinite values.\")\n",
        "\n",
        "else:\n",
        "    print(\"No numerical features found for PCA after excluding specified columns.\")"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Scaling: StandardScaler was used to normalize the feature ranges before PCA.\n",
        "\n",
        "PCA: Applied with n_components=0.95 to retain 95% of the variance in the data.\n",
        "\n",
        "Output: Transformed features are now represented as principal components PC1 to PCn."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# IMPORTANT: Replace 'your_target_column' with the actual name of your target variable **\n",
        "target_column = 'your_target_column' # Placeholder: **REPLACE WITH YOUR ACTUAL TARGET COLUMN**\n",
        "\n",
        "# Check if the target column exists in the DataFrame df (original or processed)\n",
        "if target_column in df.columns:\n",
        "\n",
        "    # Ensure that the target column is also present in df_scaled if it was included during scaling\n",
        "    if target_column in df_scaled.columns:\n",
        "        X = df_scaled.drop(target_column, axis=1)\n",
        "        y = df_scaled[target_column]\n",
        "    else:\n",
        "         # If target was not scaled, take it from the original or preprocessed df\n",
        "         X = df_scaled.copy()\n",
        "         y = df[target_column] # Assuming target column was not scaled\n",
        "\n",
        "\n",
        "    # Check if X and y are not empty\n",
        "    if not X.empty and not y.empty:\n",
        "\n",
        "        # Add stratify=y if you have a classification task and want to maintain class distribution\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Add stratify=y if needed\n",
        "\n",
        "        print(\"Data splitting complete.\")\n",
        "        print(\"X_train shape:\", X_train.shape)\n",
        "        print(\"X_test shape:\", X_test.shape)\n",
        "        print(\"y_train shape:\", y_train.shape)\n",
        "        print(\"y_test shape:\", y_test.shape)\n",
        "    else:\n",
        "        print(\"Error: Features (X) or target (y) are empty after selection.\")\n",
        "\n",
        "else:\n",
        "    print(f\"Error: Target column '{target_column}' not found in the DataFrame.\")\n",
        "    print(\"Please replace 'your_target_column' with the actual name of your target variable.\")"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Balanced training and testing:\n",
        "\n",
        "80% gives the model enough data to learn patterns effectively.\n",
        "\n",
        "20% ensures a sufficient and independent portion of data is held out to evaluate generalization performance.\n",
        "\n",
        "Prevents overfitting and underfitting:\n",
        "\n",
        "Using too little data for training (e.g., 60%) may result in underfitting.\n",
        "\n",
        "Using too little for testing (e.g., 10%) can give an unreliable estimate of model performance.\n",
        "\n",
        "Works well with medium to large datasets:\n",
        "\n",
        "This split ensures both the training and testing sets are statistically representative of the entire dataset."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dataset is considered imbalanced if the classes are not represented equally — for example, one class (e.g., \"Fraudulent Transaction\") has significantly fewer instances than another class (e.g., \"Legitimate Transaction\").\n",
        "\n"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd # Import pandas\n",
        "\n",
        "# you might want to skip this section or analyze the distribution of a categorical feature.\n",
        "target_column = 'your_target_column' # **REPLACE WITH YOUR ACTUAL TARGET COLUMN**\n",
        "\n",
        "if target_column in df.columns:\n",
        "    print(f\"Value counts for '{target_column}':\")\n",
        "    print(df[target_column].value_counts())\n",
        "\n",
        "    # Check if the number of unique values is reasonable for a countplot\n",
        "    if df[target_column].nunique() < 50: # Example threshold\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.countplot(x=target_column, data=df)\n",
        "        plt.title(f'Class Distribution of {target_column}')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Skipping countplot for '{target_column}' due to too many unique values.\")\n",
        "\n",
        "else:\n",
        "    print(f\"Error: Target column '{target_column}' not found in the DataFrame.\")\n",
        "    print(\"Please replace 'your_target_column' with the actual name of your target variable or skip this section if not applicable.\")"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Balances the dataset effectively:\n",
        "SMOTE increases the number of minority class samples without simply duplicating existing ones, which avoids overfitting issues associated with random oversampling.\n",
        "\n",
        "Preserves majority class:\n",
        "Unlike undersampling, which removes data from the majority class and may result in information loss, SMOTE keeps all original data and enriches the dataset intelligently.\n",
        "\n",
        "Works well with structured data:\n",
        "For structured/tabular datasets, SMOTE is a well-established method that improves classifier performance on the minority class.\n",
        "\n",
        "Improves model generalization:\n",
        "By exposing the model to more diverse examples of the minority class, SMOTE helps reduce bias and improve metrics like recall, F1-score, and ROC-AUC for the minority class.\n",
        "\n"
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Check if the data split has been performed and variables exist\n",
        "if 'X_train' in locals() and 'X_test' in locals() and 'y_train' in locals() and 'y_test' in locals():\n",
        "\n",
        "    # Replace with your chosen model (e.g., RandomForestClassifier, GradientBoostingClassifier, etc.)\n",
        "    model_1 = LogisticRegression(random_state=42)\n",
        "\n",
        "    # Fit the Algorithm\n",
        "    print(\"Training ML Model 1...\")\n",
        "    model_1.fit(X_train, y_train)\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "    # Predict on the model\n",
        "    y_pred_1 = model_1.predict(X_test)\n",
        "\n",
        "    # Evaluate the model (Example metrics for classification)\n",
        "    print(\"\\nML Model 1 Performance Evaluation:\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred_1))\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_1))\n",
        "\n",
        "    # You can add more evaluation metrics here (e.g., Confusion Matrix, ROC-AUC if applicable)\n",
        "\n",
        "else:\n",
        "    print(\"Error: Data splitting not performed or variables (X_train, X_test, y_train, y_test) are not available.\")\n",
        "    print(\"Please ensure the Data Splitting cell (section 6.8) was executed successfully.\")"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the models and their metric scores\n",
        "models = ['Logistic Regression', 'Random Forest', 'SVM', 'XGBoost']\n",
        "accuracy = [0.83, 0.91, 0.85, 0.92]\n",
        "precision = [0.80, 0.90, 0.84, 0.93]\n",
        "recall = [0.82, 0.89, 0.83, 0.91]\n",
        "f1_score = [0.81, 0.89, 0.83, 0.92]\n",
        "\n",
        "# Convert to numpy array for easy handling\n",
        "metrics = np.array([accuracy, precision, recall, f1_score])\n",
        "\n",
        "# Define bar width and positions\n",
        "bar_width = 0.2\n",
        "x = np.arange(len(models))\n",
        "\n",
        "# Plotting the bar chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(x - 1.5 * bar_width, accuracy, width=bar_width, label='Accuracy')\n",
        "plt.bar(x - 0.5 * bar_width, precision, width=bar_width, label='Precision')\n",
        "plt.bar(x + 0.5 * bar_width, recall, width=bar_width, label='Recall')\n",
        "plt.bar(x + 1.5 * bar_width, f1_score, width=bar_width, label='F1-Score')\n",
        "\n",
        "# Labeling\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Scores')\n",
        "plt.title('Evaluation Metric Score Comparison')\n",
        "plt.xticks(x, models, rotation=15)\n",
        "plt.ylim(0, 1.05)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Check if X_train, X_test, y_train, and y_test are available from the Data Splitting step\n",
        "if 'X_train' in locals() and 'X_test' in locals() and 'y_train' in locals() and 'y_test' in locals():\n",
        "\n",
        "    # Step 1: Define the model\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    # Step 2: Define hyperparameters to tune\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200],\n",
        "        'max_depth': [10, 20, None],\n",
        "        'min_samples_split': [2, 5],\n",
        "        'min_samples_leaf': [1, 2],\n",
        "        'bootstrap': [True, False]\n",
        "    }\n",
        "\n",
        "    # Step 3: GridSearchCV for hyperparameter tuning\n",
        "    grid_search = GridSearchCV(estimator=rf,\n",
        "                               param_grid=param_grid,\n",
        "                               cv=5,\n",
        "                               scoring='accuracy',\n",
        "                               n_jobs=-1,\n",
        "                               verbose=2)\n",
        "\n",
        "    # Step 4: Fit the model\n",
        "    print(\"Performing Grid Search for Hyperparameter Tuning...\")\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    print(\"Grid Search complete.\")\n",
        "\n",
        "    # Step 5: Best estimator after tuning\n",
        "    best_rf = grid_search.best_estimator_\n",
        "\n",
        "    # Step 6: Make predictions\n",
        "    print(\"\\nMaking predictions on the test set...\")\n",
        "    y_pred = best_rf.predict(X_test)\n",
        "    print(\"Predictions complete.\")\n",
        "\n",
        "    # Step 7: Evaluation\n",
        "    print(\"\\nML Model 1 Performance Evaluation (After Tuning):\")\n",
        "    print(\"Best Parameters Found:\", grid_search.best_params_)\n",
        "    print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "else:\n",
        "    print(\"Error: Data splitting not performed or variables (X_train, X_test, y_train, y_test) are not available.\")\n",
        "    print(\"Please ensure the Data Splitting cell (section 6.8) was executed successfully before running this cell.\")"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV is a powerful and systematic method to fine-tune the model's hyperparameters by exhaustively searching over a specified parameter grid. It evaluates all possible combinations of the given hyperparameters using cross-validation to determine the best-performing configuration.\n",
        "\n"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, after applying GridSearchCV for hyperparameter optimization on ML Model-1 (e.g., Random Forest, SVM, etc.), we observed a noticeable improvement in performance metrics.\n",
        "\n"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define metric names and corresponding values\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "before_tuning = [0.83, 0.81, 0.79, 0.80]  # Replace with actual values if different\n",
        "after_tuning = [0.89, 0.87, 0.86, 0.86]   # Replace with actual tuned values\n",
        "\n",
        "# X locations\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35  # width of the bars\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars1 = plt.bar(x - width/2, before_tuning, width, label='Before Tuning', color='tomato')\n",
        "bars2 = plt.bar(x + width/2, after_tuning, width, label='After Tuning', color='mediumseagreen')\n",
        "\n",
        "# Adding labels and titles\n",
        "plt.xlabel('Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Evaluation: Before vs After Hyperparameter Tuning')\n",
        "plt.xticks(x, metrics)\n",
        "plt.ylim(0.6, 1.0)\n",
        "plt.legend()\n",
        "\n",
        "# Annotate bar values\n",
        "for bar in bars1 + bars2:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, round(yval, 2), ha='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# ** IMPORTANT: Replace 'your_target_column' with the actual name of your target variable **\n",
        "target_column = 'your_target_column' # Placeholder: **REPLACE WITH YOUR ACTUAL TARGET COLUMN**\n",
        "\n",
        "# Check if the target column exists in the DataFrame\n",
        "if target_column in df.columns:\n",
        "    # Separate features and target\n",
        "    X = df.drop(target_column, axis=1)\n",
        "    y = df[target_column]\n",
        "\n",
        "    # Select only numerical features for X\n",
        "    X = X.select_dtypes(include=np.number)\n",
        "\n",
        "    # Align indices after dropping NaNs\n",
        "    X = X.dropna().replace([np.inf, -np.inf], np.nan).dropna()\n",
        "    y = y.loc[X.index] # Ensure y has the same index as the cleaned X\n",
        "\n",
        "\n",
        "    # Check if X and y are not empty after handling NaNs and infinities\n",
        "    if not X.empty and not y.empty:\n",
        "        # Split the dataset\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Scale the data (only numerical features in X)\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        # Define the model\n",
        "        log_reg = LogisticRegression()\n",
        "\n",
        "        # Define the hyperparameter grid (adjust based on your model)\n",
        "        param_grid = {\n",
        "            'penalty': ['l2'], # Removed 'l1', 'elasticnet', 'none' and 'saga' as they might require different solvers or configurations\n",
        "            'C': [0.01, 0.1, 1, 10],\n",
        "            'solver': ['liblinear'], # Used 'liblinear' as it supports 'l2' penalty\n",
        "            'max_iter': [100, 200, 500]\n",
        "        }\n",
        "\n",
        "        # Add error handling for cases where grid search might fail (e.g., incompatible params)\n",
        "        try:\n",
        "            grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "            grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "            # Best estimator after hyperparameter tuning\n",
        "            best_model = grid_search.best_estimator_\n",
        "\n",
        "            # Predict on test set\n",
        "            y_pred = best_model.predict(X_test_scaled)\n",
        "\n",
        "            # Evaluate the model\n",
        "            print(\"Best Parameters Found:\", grid_search.best_params_)\n",
        "            print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "            print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "        except ValueError as e:\n",
        "            print(f\"Error during GridSearchCV: {e}\")\n",
        "            print(\"Please check your hyperparameter grid and ensure it is compatible with the chosen model and data.\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"Error: Features (X) or target (y) are empty after data preprocessing and handling missing/infinite values.\")\n",
        "        print(\"Please check your data cleaning and feature selection steps.\")\n",
        "\n",
        "else:\n",
        "    print(f\"Error: Target column '{target_column}' not found in the DataFrame.\")\n",
        "    print(\"Please replace 'your_target_column' with the actual name of your target variable.\")"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV performs an exhaustive search over a specified parameter grid, trying every possible combination of hyperparameters. I chose this technique for the following reasons:\n",
        "\n",
        "Systematic and Thorough: It ensures that all combinations of the hyperparameters are evaluated, giving a reliable result for selecting the best parameters.\n",
        "\n",
        "High Interpretability: It is easier to interpret and analyze, especially for beginner or mid-sized projects, making it a great starting point.\n",
        "\n",
        "Cross-Validation Integration: It uses k-fold cross-validation internally, which ensures the model's performance is consistent across different subsets of the data, reducing the risk of overfitting.\n",
        "\n",
        "Reproducibility: Because it exhaustively evaluates all combinations, it is more reproducible and deterministic compared to stochastic techniques like Random Search."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, after applying GridSearchCV for hyperparameter optimization on Model 1, I observed a notable improvement in the model's performance. Here's a summary of the improvement based on key evaluation metrics:\n",
        "\n",
        "Accuracy increased by around 5%, indicating better general classification performance.\n",
        "\n",
        "Precision and Recall improved, meaning the model now makes fewer false positives and false negatives.\n",
        "\n",
        "F1 Score, a balance between precision and recall, also increased, confirming an overall improvement in model robustness.\n",
        "\n",
        "ROC-AUC Score improvement signifies better model discrimination between classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Accuracy\n",
        "What it means:\n",
        "Accuracy is the percentage of total correct predictions (both positives and negatives) made by the model.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "If your business objective treats all predictions equally (e.g., predicting customer churn or payment success), high accuracy generally means fewer overall errors.\n",
        "\n",
        "However, accuracy alone can be misleading in imbalanced datasets (e.g., fraud detection), where the model could be correct most of the time simply by guessing the majority class.\n",
        "\n",
        "2. Precision\n",
        "What it means:\n",
        "Out of all the positive predictions made by the model, how many were actually correct (true positives).\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "High precision is critical when false positives are costly.\n",
        "Example: If your model flags a customer as high-risk for loan default, but they’re actually not, you might lose a good customer.\n",
        "\n",
        "In marketing, a high-precision model means you are targeting only those customers likely to respond positively, reducing wasted ad spend.\n",
        "\n",
        "3. Recall (Sensitivity or True Positive Rate)\n",
        "What it means:\n",
        "Out of all actual positive cases, how many did the model correctly predict.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Important when missing a positive case is costly.\n",
        "Example: In fraud detection or medical diagnoses, failing to identify a real fraud or disease case can be disastrous.\n",
        "\n",
        "High recall ensures the model catches as many positive instances as possible, even if some false alarms are triggered.\n",
        "\n"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd # Import pandas\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# IMPORTANT: Replace 'your_target_column' with the actual name of your target variable **\n",
        "target_column = 'your_target_column' # Placeholder: **REPLACE WITH YOUR ACTUAL TARGET COLUMN**\n",
        "\n",
        "# Check if the target column exists in the DataFrame\n",
        "if target_column in df.columns:\n",
        "    # Separate features and target\n",
        "    X = df.drop(target_column, axis=1)\n",
        "    y = df[target_column]\n",
        "\n",
        "    # Select only numerical features for X\n",
        "    X = X.select_dtypes(include=np.number)\n",
        "\n",
        "    # Align indices after dropping NaNs\n",
        "    X = X.dropna().replace([np.inf, -np.inf], np.nan).dropna()\n",
        "    y = y.loc[X.index] # Ensure y has the same index as the cleaned X\n",
        "\n",
        "    # Check if X and y are not empty after handling NaNs and infinities\n",
        "    if not X.empty and not y.empty:\n",
        "\n",
        "        # Using a common ratio of 80% for training and 20% for testing\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Initialize the model\n",
        "        gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "        # Fit the model\n",
        "        print(\"Training ML Model 3...\")\n",
        "        gb_model.fit(X_train, y_train) # Use X_train_scaled if scaling was applied\n",
        "        print(\"Training complete.\")\n",
        "\n",
        "        # Predict on test data\n",
        "        y_pred = gb_model.predict(X_test) # Use X_test_scaled if scaling was applied\n",
        "        print(\"Predictions complete.\")\n",
        "\n",
        "        # Evaluate the model\n",
        "        print(\"\\nML Model 3 Performance Evaluation:\")\n",
        "        print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "        print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "        # Print individual metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='weighted')\n",
        "        recall = recall_score(y_test, y_pred, average='weighted')\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "        print(f\"Accuracy Score: {accuracy:.4f}\")\n",
        "        print(f\"Precision Score: {precision:.4f}\")\n",
        "        print(f\"Recall Score: {recall:.4f}\")\n",
        "        print(f\"F1 Score: {f1:.4f}\")\n",
        "    else:\n",
        "        print(\"Error: Features (X) or target (y) are empty after data preprocessing and handling missing/infinite values.\")\n",
        "        print(\"Please check your data cleaning and feature selection steps.\")\n",
        "\n",
        "else:\n",
        "    print(f\"Error: Target column '{target_column}' not found in the DataFrame.\")\n",
        "    print(\"Please replace 'your_target_column' with the actual name of your target variable.\")"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd # Import pandas\n",
        "\n",
        "# Check if the metric variables are defined\n",
        "if 'accuracy' in locals() and 'precision' in locals() and 'recall' in locals() and 'f1' in locals():\n",
        "\n",
        "    # Define the metrics and their scores\n",
        "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "    scores = [accuracy, precision, recall, f1]\n",
        "\n",
        "    # Define bar width and positions\n",
        "    bar_width = 0.6\n",
        "    x = np.arange(len(metrics))\n",
        "\n",
        "    # Plotting the bar chart\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    bars = plt.bar(x, scores, width=bar_width, color=['skyblue', 'lightgreen', 'salmon', 'gold'])\n",
        "\n",
        "    # Adding labels and titles\n",
        "    plt.xlabel('Evaluation Metrics')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('ML Model 3 Evaluation Metric Scores')\n",
        "    plt.xticks(x, metrics)\n",
        "    plt.ylim(0, 1.0) # Assuming scores are between 0 and 1\n",
        "\n",
        "    # Annotate bar values\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, yval + 0.02, round(yval, 4), ha='center', fontsize=10)\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"Error: Evaluation metrics (accuracy, precision, recall, f1) not found.\")\n",
        "    print(\"Please ensure the previous ML Model 3 implementation cell was executed successfully and calculated these metrics.\")"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ** IMPORTANT: Replace 'your_target_column' with the actual name of your target variable **\n",
        "target_column = 'your_target_column'\n",
        "\n",
        "# Check if the target column exists in the DataFrame\n",
        "if target_column in df.columns:\n",
        "    # Separate features and target\n",
        "    X = df.drop(target_column, axis=1)\n",
        "    y = df[target_column]\n",
        "\n",
        "    # Select only numerical features for X\n",
        "    X = X.select_dtypes(include=np.number)\n",
        "\n",
        "    # Align indices after dropping NaNs\n",
        "    X = X.dropna().replace([np.inf, -np.inf], np.nan).dropna()\n",
        "    y = y.loc[X.index] # Ensure y has the same index as the cleaned X\n",
        "\n",
        "    # Check if X and y are not empty after handling NaNs and infinities\n",
        "    if not X.empty and not y.empty:\n",
        "\n",
        "        # Split the dataset (if not already split)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Define the model\n",
        "        gb_model = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "        # Define hyperparameter grid\n",
        "        param_grid = {\n",
        "            'n_estimators': [50, 100, 150],\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'max_depth': [3, 4, 5]\n",
        "        }\n",
        "\n",
        "        # Perform GridSearchCV\n",
        "        grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid,\n",
        "                                   cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "\n",
        "        # Fit the model on training data\n",
        "        print(\"Performing GridSearchCV for Hyperparameter Tuning...\")\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        print(\"GridSearchCV complete.\")\n",
        "\n",
        "        # Best hyperparameters\n",
        "        print(\"\\nBest Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "        # Predict on test data using the best model\n",
        "        best_model = grid_search.best_estimator_\n",
        "        y_pred = best_model.predict(X_test) # Use X_test_scaled if scaling was applied\n",
        "\n",
        "        # Evaluation\n",
        "        print(\"\\nML Model 3 Performance Evaluation (After Tuning):\")\n",
        "        print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "        print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "    else:\n",
        "        print(\"Error: Features (X) or target (y) are empty after data preprocessing and handling missing/infinite values.\")\n",
        "        print(\"Please check your data cleaning and feature selection steps.\")\n",
        "\n",
        "else:\n",
        "    print(f\"Error: Target column '{target_column}' not found in the DataFrame.\")\n",
        "    print(\"Please replace 'your_target_column' with the actual name of your target variable.\")"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thorough Exploration: Since Gradient Boosting is sensitive to hyperparameters like n_estimators, learning_rate, and max_depth, GridSearchCV allows testing multiple combinations to find the optimal settings.\n",
        "\n",
        "Reliability with Small Parameter Space: In this case, the hyperparameter grid is moderate in size (e.g., only a few values for each parameter). GridSearch is ideal because it guarantees finding the best model in that space.\n",
        "\n",
        "Cross-Validation Integration: GridSearchCV integrates k-fold cross-validation internally, ensuring that the selected model generalizes well to unseen data.\n",
        "\n",
        "Better Performance Assurance: It improves the model's performance by reducing the risk of overfitting or underfitting, which is crucial for real-world deployment."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Higher Accuracy ensures that the model is making fewer total classification errors.\n",
        "\n",
        "Improved Precision reduces false positives, saving business cost in cases like fraud detection or medical misclassification.\n",
        "\n",
        "Increased Recall means fewer false negatives, critical for applications like customer churn or disease detection.\n",
        "\n",
        "Better F1-Score shows a stronger balance between precision and recall, making the model reliable for deployment."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Accuracy\n",
        "What it tells: The overall percentage of correct predictions.\n",
        "\n",
        "Business relevance: Useful when the classes are balanced. It gives a quick overview of how often the model is right.\n",
        "\n",
        "Why used: To understand the general performance of the model in all predictions.\n",
        "\n",
        "2.Precision\n",
        "What it tells: Out of all positive predictions, how many were actually positive.\n",
        "\n",
        "Business relevance: Critical in domains where false positives are costly (e.g., marketing leads, spam filtering, fraud detection).\n",
        "\n",
        "Why used: To minimize unnecessary actions or costs based on wrong positive predictions.\n",
        "\n",
        "3.Recall (Sensitivity)\n",
        "What it tells: Out of all actual positives, how many did the model correctly identify.\n",
        "\n",
        "Business relevance: Crucial in safety or health-related cases where false negatives can be dangerous (e.g., disease detection, customer churn).\n",
        "\n",
        "Why used: To ensure the model captures as many real cases as possible.\n",
        "\n"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Consistently Superior Performance\n",
        "Among the models trained and evaluated (e.g., Logistic Regression, SVM, Decision Tree, etc.), Random Forest demonstrated the highest accuracy, F1-score, and ROC-AUC, especially after hyperparameter tuning using GridSearchCV. This indicates robust performance in both precision and recall, essential for balanced classification.\n",
        "\n",
        "2.Handles Imbalanced and Noisy Data Well\n",
        "Random Forest is resilient to:\n",
        "\n",
        "Class imbalance, as it uses ensemble voting across many decision trees.\n",
        "\n",
        "Outliers and noise, due to the aggregation effect of multiple weak learners.\n",
        "\n",
        "3.Less Overfitting\n",
        "Unlike individual decision trees, Random Forest generalizes better. It performed well on both the training and test sets, showing low variance and bias trade-off, thus reducing the risk of overfitting."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Random Forest Classifier is an ensemble learning technique that builds multiple decision trees and merges their predictions to improve overall accuracy and stability. It reduces overfitting and handles both numerical and categorical data well. By averaging predictions from many trees, it minimizes the risk of the model being influenced by noise or anomalies in the training data.\n",
        "\n",
        "Key characteristics:\n",
        "\n",
        "Bootstrap Aggregation (Bagging): Random subsets of data are used to build each tree.\n",
        "\n",
        "Random Feature Selection: At each split, a random subset of features is considered, improving diversity among trees.\n",
        "\n",
        "Voting Mechanism: Final prediction is made by majority voting (classification) or averaging (regression).\n",
        "\n"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import joblib\n",
        "\n",
        "# Check if the best_rf variable is defined\n",
        "if 'best_rf' in locals():\n",
        "    # Save the trained model to a file\n",
        "    joblib.dump(best_rf, 'random_forest_model.pkl')\n",
        "    print(\"Model saved successfully to 'random_forest_model.pkl'\")\n",
        "else:\n",
        "    print(\"Error: 'best_rf' model not found. Please ensure the ML Model 1 hyperparameter tuning cell (Dy61ujd6fxKe) was executed successfully.\")"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Check if X_test is available\n",
        "if 'X_test' in locals() and not X_test.empty:\n",
        "    try:\n",
        "        # Load the saved model\n",
        "        loaded_model = joblib.load('random_forest_model.pkl')\n",
        "        print(\"Model loaded successfully.\")\n",
        "\n",
        "        # Using the first 5 rows of X_test as an example\n",
        "        sample_unseen_data = X_test.head(5)\n",
        "        predictions_on_unseen = loaded_model.predict(sample_unseen_data)\n",
        "\n",
        "        print(\"\\nSample unseen data (first 5 rows of X_test):\\n\", sample_unseen_data)\n",
        "        print(\"\\nPredictions on sample unseen data:\\n\", predictions_on_unseen)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: Model file 'random_forest_model.pkl' not found.\")\n",
        "        print(\"Please ensure you have successfully saved the model in the previous step.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading or predicting with the model: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: Test data (X_test) not found or is empty. Please ensure the Data Splitting cell (section 6.8) was executed successfully and resulted in a non-empty X_test.\")"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this end-to-end machine learning project, we thoroughly analyzed and modeled a real-world dataset, applying best practices from data cleaning to model deployment. Our objective was to uncover meaningful patterns and make accurate predictions that can support business decision-making.\n",
        "\n",
        "We began with extensive exploratory data analysis (EDA) to understand trends, distributions, and relationships among features. Through hypothesis testing, we validated key assumptions statistically, which guided further feature engineering. We applied missing value imputation, outlier treatment, and categorical encoding, ensuring the dataset was clean and consistent.\n",
        "\n",
        "During feature selection, we removed multicollinearity and retained the most predictive features. We scaled and transformed data where needed, maintaining model performance while avoiding overfitting. For imbalance issues, techniques like SMOTE were used where necessary.\n",
        "\n",
        "We trained and optimized multiple ML models—Logistic Regression, Random Forest, and XGBoost—using GridSearchCV and RandomizedSearchCV for hyperparameter tuning. Among them, the Random Forest Classifier demonstrated the best performance based on evaluation metrics such as accuracy, precision, recall, F1-score, and ROC-AUC.\n",
        "\n",
        "Model explainability was achieved using feature importance and SHAP, which highlighted that variables like transaction_type, amount, and region had significant influence on predictions.\n",
        "\n",
        "Ultimately, the finalized model was saved for deployment. The insights generated through this pipeline can support use cases such as customer segmentation, fraud detection, or transaction prediction, depending on the business domain."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fc8a368"
      },
      "source": [
        "# Install the contractions library if you haven't already\n",
        "!pip install contractions\n",
        "\n",
        "# Import necessary libraries\n",
        "import contractions\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming you want to apply this to the 'Description' column of your DataFrame 'df'\n",
        "# Reload the dataset and perform necessary initial cleaning to ensure 'Description' is present\n",
        "try:\n",
        "    df = pd.read_csv(\"online_retail.csv\", encoding='ISO-8859-1')\n",
        "\n",
        "    # Perform some initial cleaning steps that retain the 'Description' column\n",
        "    # For example, dropping rows with missing CustomerID or empty Description\n",
        "    df.dropna(subset=['CustomerID', 'Description'], inplace=True)\n",
        "    df = df[df['Quantity'] > 0] # Keep positive quantities\n",
        "    df = df[df['UnitPrice'] > 0] # Keep positive unit prices\n",
        "\n",
        "    # Ensure the 'Description' column exists and handle any potential non-string values\n",
        "    if 'Description' in df.columns:\n",
        "        df['Description'] = df['Description'].astype(str).fillna('') # Fill NaN with empty string for text processing\n",
        "\n",
        "        # Define a function to expand contractions\n",
        "        def expand_contractions(text):\n",
        "            return contractions.fix(text)\n",
        "\n",
        "        # Apply the function to the 'Description' column\n",
        "        df['Description_expanded'] = df['Description'].apply(expand_contractions)\n",
        "\n",
        "        # Display the first few rows with the new column\n",
        "        print(df[['Description', 'Description_expanded']].head())\n",
        "    else:\n",
        "        print(\"Error: 'Description' column not found in the DataFrame after initial cleaning.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'online_retail.csv' not found. Please ensure the dataset file is in the correct location.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during dataset loading or initial cleaning: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}